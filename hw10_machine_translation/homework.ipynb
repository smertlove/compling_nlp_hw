{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dba7c0d",
   "metadata": {},
   "source": [
    "# Домашнее задание № 10. Машинный перевод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yj7aripVIsbG",
   "metadata": {
    "id": "Yj7aripVIsbG"
   },
   "source": [
    "## Задание 1 (6 баллов + 2 доп балла).\n",
    "Нужно обучить трансформер на том же корпусе но в другую сторону - с русского на английский.\n",
    "Можно использовать как основу первый или второй способ реализации (с MultiheadAttention или с nn.Transformer). Подберите несколько тестовых примеров для проверки обучения на каждой эпохе. \n",
    "\n",
    "Параметры ниже точно работают в колабе и модель обучается достаточно быстро. Попробуйте их немного увеличить (batch size возможно придется наоборот уменьшить). Обучайте модель хотя бы 5 эпох, а желательно больше, чтобы тестовые примеры начали переводиться более менее адекватно. \n",
    "\n",
    "После обучения возьмите хотя бы 100 примером из тестовой части параллельного корпуса и переведите их. Оцените качество переводов с помощью метрики BLEU (пример использования ниже)\n",
    "Найдите лучшие (как минимум 5) переводы согласно этой метрике и проверьте действительно ли они хорошие. Если все переводы нулевые, то пообучайте модель подольше.\n",
    "\n",
    "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Сейчас она работает только с одним текстом - это не эффективно. Можно генерировать переводы сразу для нескольких текстов (батча). Главная сложность с таким подходом состоит в том, что генерируемые тексты будут заканчиваться в разное время и нужно сделать столько итераций, сколько нужно для завершения всех текстов (т.е. условие на то, что последний токен не равен [EOS] в текущем коде не сработает). \n",
    "ВАЖНО - недостаточно просто изменить входной аргумент с text на texts и добавить еще один цикл по texts! Сама модель должна вызываться на нескольких текстах! Функция с batch prediction должна работать быстрее, поэтому переведите всю тестовую выборку и оцените качество BLEU на всех данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f491f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "from torch.nn import Transformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfecc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf2ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('./data/en-ru-train.ru').read().replace('\\xa0', ' ')\n",
    "f = open('./data/en-ru-train.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27435a1b",
   "metadata": {},
   "source": [
    "<s>Я честно пытался сделать нормально, переписать где-то имена переменных и пр, но постоянно возникала ошибка с тем, что я получал вывод из translate на уровне\n",
    "\"Привет\" > \"G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G\"\n",
    "поэтому</s> Теперь модель должна заработать, но я всё равно обучу пару ru-en взяв полностью код из семинара и применив небольшой трюк.\n",
    "Он заключается в том, чтобы все имена переменных и весь код оставить в неизменном виде и поменять только именя файлов для тренировочных датасетов токенизаторов и модели. Таким образов в en-переменных окажутся токенизатор, сет, векторы, матрицы и пр, связанные с русским языком, а в ru-переменных -- с английским. Просто уже хочется перейти к интересной части домашки, а не заниматься переименовыванием переменных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1444e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = open('./data/en-ru-train.ru').read().splitlines()  #  <-- Трюк\n",
    "ru_sents = open('./data/en-ru-train.en').read().splitlines()  #  <-- Трюк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439d0cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load existing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer_en = Tokenizer.from_file(\"./data/tokenizer_en\")\n",
    "    tokenizer_ru = Tokenizer.from_file(\"./data/tokenizer_ru\")\n",
    "    print(\"load existing\")\n",
    "except Exception:\n",
    "    tokenizer_en = Tokenizer(BPE())\n",
    "    tokenizer_en.pre_tokenizer = Whitespace()\n",
    "    trainer_en = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
    "    tokenizer_en.train(\n",
    "        files=[\"./data/en-ru-train.ru\"],  #  <-- Трюк\n",
    "        trainer=trainer_en\n",
    "    )\n",
    "\n",
    "    tokenizer_ru = Tokenizer(BPE())\n",
    "    tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "    trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
    "    tokenizer_ru.train(\n",
    "        files=[\"./data/en-ru-train.en\"],  #  <-- Трюк\n",
    "        trainer=trainer_ru\n",
    "    )\n",
    "\n",
    "    tokenizer_en.decoder = decoders.BPEDecoder()\n",
    "    tokenizer_ru.decoder = decoders.BPEDecoder()\n",
    "\n",
    "    tokenizer_en.save('./data/tokenizer_en')\n",
    "    tokenizer_ru.save('./data/tokenizer_ru')\n",
    "    print(\"create new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10bc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, max_len, encoder=False):\n",
    "    if encoder:\n",
    "        return tokenizer.encode(text).ids[:max_len]\n",
    "    else:\n",
    "        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e3ea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "# у нас это в любом случае ноль но лучше safe than sorry\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c243d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_en, max_len_ru = 47, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcffd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en, max_len_en, encoder=True) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2d6aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1000000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_en), len(X_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78736097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts_en, texts_ru):\n",
    "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
    "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
    "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "        self.length = len(texts_en)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ids_en = self.texts_en[index]\n",
    "        ids_ru = self.texts_ru[index]\n",
    "\n",
    "        return ids_en, ids_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb8eb86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_ru_train, X_ru_valid = train_test_split(X_en, X_ru, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9dd5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
    "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
    "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "\n",
    "        src_embedded = self.embedding_enc(src)\n",
    "        B,S,E = src_embedded.shape\n",
    "        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "\n",
    "        tgt_embedded = self.embedding_dec(tgt)\n",
    "        B,S,E = tgt_embedded.shape\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "\n",
    "\n",
    "        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n",
    "\n",
    "        encoder_output = self.transformer.encoder(\n",
    "            src_embedded,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        decoder_output = self.transformer.decoder(\n",
    "            tgt_embedded,\n",
    "            encoder_output,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        output = self.output_layer(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87809664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, iterator, optimizer, criterion, scheduler, run=None, print_every=100):\n",
    "\n",
    "    epoch_loss = []\n",
    "    ac = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "        texts_en = texts_en.to(DEVICE)\n",
    "        texts_ru = texts_ru.to(DEVICE)\n",
    "        texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
    "        texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
    "        src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
    "        tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "\n",
    "        logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        B,S,C = logits.shape\n",
    "        loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {np.mean(epoch_loss)};')\n",
    "        if run is not None:\n",
    "            run.log({\"loss\": loss.item()})\n",
    "\n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, run=None):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_f1 = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "            texts_en = texts_en.to(DEVICE)\n",
    "            texts_ru = texts_ru.to(DEVICE)\n",
    "            texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
    "            texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
    "            src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
    "            tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "            logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "            B,S,C = logits.shape\n",
    "            loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
    "            epoch_loss.append(loss.item())\n",
    "            if run is not None:\n",
    "                run.log({\"val_loss\": loss.item()})\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f841373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def translate(text):\n",
    "\n",
    "\n",
    "    input_ids = tokenizer_en.encode(text).ids[:max_len_en]\n",
    "    output_ids = [tokenizer_ru.token_to_id('[BOS]')]\n",
    "\n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "\n",
    "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
    "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "\n",
    "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "    pred = logits.argmax(2).item()\n",
    "\n",
    "    while pred not in [tokenizer_ru.token_to_id('[EOS]'), tokenizer_ru.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
    "        output_ids.append(pred)\n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "        pred = logits.argmax(2).view(-1)[-1].item()\n",
    "\n",
    "    return tokenizer_ru.decoder.decode([tokenizer_ru.id_to_token(i) for i in output_ids[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c97e741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderDecoder(\n",
       "  (embedding_enc): Embedding(30000, 256)\n",
       "  (embedding_dec): Embedding(30000, 256)\n",
       "  (positional_encoding): RotaryPositionalEmbeddings()\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=256, out_features=30000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# попробуйте поставить параметры поменьше если в колабе обучается слишком долго!\n",
    "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
    "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
    "embed_dim = 256 # еще называется D_MODEL\n",
    "num_heads = 8\n",
    "ff_dim = embed_dim*4 # еще называется D_FF\n",
    "num_layers = 4 # количество слоев\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "model = torch.load('model', weights_only=False)\n",
    "model.eval()\n",
    "# model = TransformerEncoderDecoder(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fab768e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I love my mother .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Я люблю маму\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e5669",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(X_en_train, X_ru_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
    "\n",
    "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4196fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x78d1a58c6dd0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAGdCAYAAAD5ZcJyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWNNJREFUeJzt3Xlc1HX+B/DXHMwM5yAiDCgqnnijIIhnm/wWi3WlrNTMg7wyLV1317JD27aW0totyzKtPPLMDitTWxcrV0UExAPv+0AHRGSG+5j5/P5AJmdFBQW+32Fez8djHq7f72eY9/fr5rz8fD+HQgghQERERNTIKaUugIiIiKghMPQQERGRU2DoISIiIqfA0ENEREROgaGHiIiInAJDDxERETkFhh4iIiJyCgw9RERE5BTUUhcgJ1arFZcvX4anpycUCoXU5RAREVENCCGQn5+PwMBAKJW3789h6LnJ5cuXERQUJHUZREREdA8uXryIFi1a3PY8Q89NPD09AVTeNC8vL4mrISIiopowm80ICgqyfY/fDkPPTaoeaXl5eTH0EBEROZi7DU3hQGYiIiJyCgw9RERE5BQYeoiIiMgpMPQQERGRU2DoISIiIqfA0ENEREROgaGHiIiInAJDDxERETkFhh4iIiJyCvcUehYtWoTWrVtDp9MhMjISe/fuvWP7DRs2ICQkBDqdDt26dcPmzZvtzgshMHfuXAQEBMDV1RXR0dE4efKkXZs333wTffv2hZubG7y9vav9nAsXLiA2NhZubm7w8/PDX//6V1RUVNzLJRIREVEjU+vQs379esyaNQvz5s3Dvn370KNHD8TExCA7O7va9rt378aoUaMwYcIEpKenIy4uDnFxccjIyLC1mT9/PhYuXIjFixcjOTkZ7u7uiImJQUlJia1NWVkZHn/8cUydOrXaz7FYLIiNjUVZWRl2796NFStWYPny5Zg7d25tL5GIiIgaI1FLERERYtq0abbfWywWERgYKBISEqpt/8QTT4jY2Fi7Y5GRkWLKlClCCCGsVqswGAxiwYIFtvN5eXlCq9WKtWvX3vLzli1bJvR6/S3HN2/eLJRKpTAajbZjH3/8sfDy8hKlpaU1ujaTySQACJPJVKP2REREJL2afn/XasPRsrIypKWlYc6cObZjSqUS0dHRSEpKqvY9SUlJmDVrlt2xmJgYbNy4EQBw9uxZGI1GREdH287r9XpERkYiKSkJI0eOrFFtSUlJ6NatG/z9/e0+Z+rUqTh8+DB69ux5y3tKS0tRWlpq+73ZbK7RZ8lFTkEpVu05j8LSCriolFCrlNCoFHBRKeGpc4He9beXt5sLmnlqoXNRSV02ERGRJGoVenJycmCxWOyCBQD4+/vj2LFj1b7HaDRW295oNNrOVx27XZuauN3n3PwZ/yshIQF/+9vfavwZciKEQPyyFBzKNNXqfU3dNQjw1iFA74pAvQ7Bvu5o6+eBts08YPDSQam88w61REREjqpWoaexmTNnjl0vlNlsRlBQkIQV1dyeM7k4lGmCUgGM69saCihQbrGi3GJFWYUV5pIKmIvLkVdcBlNxOa4XlaOswoprhWW4VliGjMxbe7XcNCq0aeaOLgF6dG2hR7fmeoQYPNk7REREjUKtQo+vry9UKhWysrLsjmdlZcFgMFT7HoPBcMf2Vb9mZWUhICDArk1oaGiNazMYDLfMIqv63NvVptVqodVqa/wZcvLL8cqB44/0bIF5Q7vctb0QAnlF5bhsKsaVvBJcMZcg83oxzlwtwOmrBTh/rQhFZRZkZJqRkWnG+tSLAAC1UoGOBk+Et2qCyDZNERHsA18Px7xnRETk3GoVejQaDcLCwpCYmIi4uDgAgNVqRWJiIqZPn17te6KiopCYmIiZM2fajm3btg1RUVEAgODgYBgMBiQmJtpCjtlsRnJy8m1nat3uc958801kZ2fDz8/P9jleXl7o3LlzbS7TIew5mwsAGNjBt0btFQoFmrhr0MRdgy6B+lvOl1usuJhbhBNZBcjINOHQjVduYRkOXzbj8GUzViSdBwC08/NAZLAPBnZohn7tfOGhdeoOQyIichC1/raaNWsWxo0bh/DwcEREROC9995DYWEh4uPjAQBjx45F8+bNkZCQAACYMWMGBg0ahHfffRexsbFYt24dUlNTsWTJEgCVX8YzZ87EG2+8gfbt2yM4OBivvvoqAgMDbcEKqFyDJzc3FxcuXIDFYsH+/fsBAO3atYOHhwd+//vfo3PnzhgzZgzmz58Po9GIV155BdOmTXPY3pzbsVoFTmblAwC6Nr81wNwLF5USbZp5oE0zDwzpWtkzJoTAZVMJ9l/Iw96z15B8NhfHjPk4lV2AU9kFWJ18AS4qBXq39sHvOvrhdyHN0LaZBxQKjgsiIiL5qXXoGTFiBK5evYq5c+fCaDQiNDQUW7dutQ0avnDhApTK35b/6du3L9asWYNXXnkFL730Etq3b4+NGzeia9eutjazZ89GYWEhJk+ejLy8PPTv3x9bt26FTqeztZk7dy5WrFhh+33VbKyff/4ZDzzwAFQqFTZt2oSpU6ciKioK7u7uGDduHF5//fXa3xWZy8wrRlGZBRqVEq183OrtcxQKBZp7u6K5tytiu1c+eswrKsPes7nYffoafj6ejfPXirD79DXsPn0Nb24+irbN3BHbLQAPdw9AR39PBiAiIpINhRBCSF2EXJjNZuj1ephMJnh5eUldzm3950gWJq5MRacAL2yZMUDSWs7mFOLnY9n4+Xg2ks/kosxitZ1r08wdf+gWgLiezdGmmYeEVRIRUWNW0+9vDsZwQMdvPNrq6C99kAj2dUdw/2A83T8Y5pJyJB7Nwo8Hjdhx4irOXC3Ewu2nsHD7KYS3aoLHwlogtnsAPHUuUpdNREROiKHHAVWN52nv7ylxJfa8dC54pGcLPNKzBfJLypF4NBsb92dix4mrSD1/Hannr+O1Hw7joa4BGNk7CBHBPnz8RUREDYahxwFdul4MAGjd1F3iSm7PU+eCuJ7NEdezObLMJfg2PRMbUi/i9NVCfJueiW/TMxFi8MTYqNaI6xkINw3/r0hERPWLY3pu4ihjevomJOKyqQTfPtsXPVs2kbqcGhNCYP/FPHyZehEb0y+juNwCAPDSqfFEeBDGRLVCKxkHOSIikqeafn8z9NzEEUJPhcWKDq9sgVUAe18aDD8v3d3fJEOmonJsSLuIL/acx/lrRQAAhQJ4qKsBzwxqi+4tvKUtkIiIHAYHMjdSWfmlsArARaVw6JWR9W4umDigDZ7uF4xfT17Fit3n8Mvxq9h8yIjNh4zo384Xzwxqi37tmnLcDxER1QmGHgdzOa9yPI9B3zg2B1UqFZULG3b0w3FjPj759TS+O3AZO0/lYOepHHRrrsdzD7bD/3X2Z/ghIqL7orx7E5KTqtATqHeVuJK619HgiX+OCMWvf30A4/u2hs5FiUOZJkz+Ig3DFu3CL8ezwaexRER0rxh6HMzlvBIAQHPvxhd6qrRo4obX/tgFu154EM8+0BZuGhUOXjJh/LIUPLY4CbtP50hdIhEROSCGHgdzxVTZ0xPg7ZgDmGujqYcWs4eEYMfs32HSgGBo1Uqknb+OJ5cm48mle5CRaZK6RCIiciAMPQ4mp6AUANDMgQcx15avhxYvx3bGjtm/w7ioVtColNh9+hqGfrgTs77cbwuCREREd8LQ42By8ssAVPaCOBt/Lx3+Nqwrtv9lEIaFBkII4Jt9mfjdO7/g3X8fR0FphdQlEhGRjDH0OJiqnh5Hnq5+v1o0ccP7I3viu2n9ENHaByXlVnyw/RQeWPALNqRehNXKwc5ERHQrhh4HY3u85amRuBLp9QjyxvopffDJmDAE+7ojp6AUf/3qIB7/JAmHL3O8DxER2WPocSClFRaYSyof4ThzT8/NFAoFYroY8NPMgXjp4RC4aVRIO38dQz/Yide+PwxTcbnUJRIRkUww9DiQawWV43lcVAroXV0krkZeNGolJg9si8Q/D0Js9wBYBbB89zkMfvcXfJ12iev7EBERQ48jqXq01dRdy9WJbyNA74pFT/bCqgmRaNPMHTkFZfjzhgN46rNkXLixxxcRETknhh4HUtXT09SD43nupn97X2ydMRCzh3SEVq3ErlPXEPPeDnz63zOwcKAzEZFTYuhxIFc5c6tWNGolnn2gHX6aORB92viguNyCN348ikc/3o3jxnypyyMiogbG0ONA2NNzb1r7umPNxD5IeLQbPLVqHLiYhz988F/8a9sJlFusUpdHREQNhKHHgeQVV4YeHzeGntpSKhUYFdES22YNQnQnf5RbBN5PPIlHPtqFk1ns9SEicgYMPQ7EVFQ5/drbjTO37pVBr8PSsWH4YFRPeLu5ICPTjNgPduKznWe5qCERUSPH0ONA8m6EHj17eu6LQqHA0B6B+GnmQAzq0AxlFVb8fdMRPPVZMjLzuI8XEVFjxdDjQKoeb3lzjZ464e+lw/L43ngjritcXVTYffoahry3AxvTM6UujYiI6gFDjwPJ4+OtOqdQKPBUn1bYPGMAQoO8kV9SgZnr92PWl/tRyA1MiYgaFYYeB1K1pYK3Kx9v1bVgX3d89UwU/hTdAUpF5e7tQz/YyT28iIgaEYYeB8KenvqlVikxI7o91k2OgsFLhzM5hXjko91YmXSO21gQETUCDD0OoqTcguJyCwBAz9BTryKCfbBlxgBEd/JDWYUVc787jClfpCGvqEzq0oiI6D4w9DgI841HWyqlAp5atcTVNH5N3DVYOjYcc//QGS4qBf59JAuxC3ci/cJ1qUsjIqJ7xNDjIPJuhB69qws3G20gCoUCT/cPxjdT+6FVUzdk5hVjxCd7sGrPeT7uIiJyQAw9DsI2nofT1RtctxZ6bHquP4Z0MaDMYsUrGzPwlw0HUXLjcSMRETkGhh4HUTWehON5pOGpc8HHT/XCnIdCoFQAX++7hEc/2o0L14qkLo2IiGqIocdB3Px4i6ShUCgwZVBbrJoQiabuGhy5YsbQD3fi5+PZUpdGREQ1wNDjIEx8vCUbfdv5YtPz/REa5A1TcTmeXp6ChYknOc6HiEjmGHochG0LCu67JQsBelesn9IHY/q0ghDAP7edwPS16Sgu4zgfIiK5YuhxELbNRtnTIxtatQp/j+uKt4d3g4tKgR8PXsHjn+zGZW5aSkQkSww9DqJqTA9XY5afEb1bYvXEPmjqrkFGphl//HAX9nE9HyIi2WHocRAmbkEhaxHBPvhuej+EGDyRU1CKkZ/swddpl6Qui4iIbsLQ4yBMnL0ley2auOHrqX0R08UfZRYr/rzhABI2H4XVygHORERywNDjIBh6HIO7Vo2PR4fh+QfbAQA+2XEGz67exwHOREQywNDjIApKKwBULpJH8qZUKjDr9x3x/shQaFRKbD1sxKile5BTUCp1aURETo2hxwEIIZBfUtnT48HNRh3GsNDmWDUxEnpXF+y/mIdHPtqF01cLpC6LiMhpMfQ4gNIKK8otleNCPHUMPY4kItgH3zzbFy193HAxtxiPfrQbyWeuSV0WEZFTYuhxAPkllY+2FArAXcPQ42jaNvPAt8/2Rc+WlSs4j/lsL77bnyl1WUREToehxwHc/GhLqVRIXA3di6YeWqyd1AcPda3cqX3Guv1Y9PMpbl1BRNSAGHocQFVPjyfH8zg0nYsKi57shckD2wAAFvx0HH/74QintBMRNRCGHgdgCz2cueXwlEoFXnq4E+YN7QwAWL77HGas34+yCqvElRERNX4MPQ6goLTy8RYHMTce8f2C8f7IUKiVCvxw4DImrEixLUtARET1g6HHAZhv9PR4MPQ0KsNCm+Pz8b3hplHhvydz8OTSPbjGtXyIiOoNQ48D4OOtxmtgh2ZYM6kPmri54OAlEx5fnISLuUVSl0VE1Cgx9DiAAlvoYU9PYxQa5I2vpvZFc29XnMkpxPCPd+OY0Sx1WUREjQ5DjwOomrLO0NN4tW3mga+n9kVHf09k55di5JI9OHgpT+qyiIgaFYYeB8Ap687BoNfhyylRCA3yRl5ROZ5cmoy9Z3OlLouIqNFg6HEA3GzUeejdXLBqYiT6tPFBQWkFxn6ejB0nrkpdFhFRo8DQ4wDMfLzlVDy0aiyPj8ADHZuhpNyKiStS8dNho9RlERE5PIYeB8DZW85H56LCkjHhtm0rnl29DxvTuV8XEdH9YOhxADfvvUXOQ6NW4oNRPTG8VwtYrAJ/+nI/1iRfkLosIiKHdU+hZ9GiRWjdujV0Oh0iIyOxd+/eO7bfsGEDQkJCoNPp0K1bN2zevNnuvBACc+fORUBAAFxdXREdHY2TJ0/atcnNzcXo0aPh5eUFb29vTJgwAQUFBXZtfvrpJ/Tp0weenp5o1qwZhg8fjnPnzt3LJcrKb2N6GHqcjVqlxILHumNMn1YQAnjp20P4bOdZqcsiInJItQ4969evx6xZszBv3jzs27cPPXr0QExMDLKzs6ttv3v3bowaNQoTJkxAeno64uLiEBcXh4yMDFub+fPnY+HChVi8eDGSk5Ph7u6OmJgYlJSU2NqMHj0ahw8fxrZt27Bp0ybs2LEDkydPtp0/e/Yshg0bhgcffBD79+/HTz/9hJycHDz66KO1vUTZqXq85cXHW05JqVTg9WFdMGVQ5Ualf990BEt3nJG4KiIiByRqKSIiQkybNs32e4vFIgIDA0VCQkK17Z944gkRGxtrdywyMlJMmTJFCCGE1WoVBoNBLFiwwHY+Ly9PaLVasXbtWiGEEEeOHBEAREpKiq3Nli1bhEKhEJmZmUIIITZs2CDUarWwWCy2Nt9//71QKBSirKysRtdmMpkEAGEymWrUviGUV1hEqxc2iVYvbBLXCkqlLockZLVaxbs/HbP9/2HxL6ekLomISBZq+v1dq56esrIypKWlITo62nZMqVQiOjoaSUlJ1b4nKSnJrj0AxMTE2NqfPXsWRqPRro1er0dkZKStTVJSEry9vREeHm5rEx0dDaVSieTkZABAWFgYlEolli1bBovFApPJhC+++ALR0dFwcam+h6S0tBRms9nuJTeFpRbb/+bjLeemUCgw6/cdMTO6PQAgYcsxfPzLaYmrIiJyHLUKPTk5ObBYLPD397c77u/vD6Ox+im1RqPxju2rfr1bGz8/P7vzarUaPj4+tjbBwcH497//jZdeeglarRbe3t64dOkSvvzyy9teT0JCAvR6ve0VFBR0t1vQ4Kqmq+tclHBRcdw5ATOjO+BP0R0AAG9vPYZFP5+SuCIiIsfQaL5FjUYjJk2ahHHjxiElJQW//vorNBoNHnvsMQghqn3PnDlzYDKZbK+LFy82cNV3VzWI2UPL8Tz0mxnR7fHn/6sMPgt+Oo4Pt5+8yzuIiKhWz0t8fX2hUqmQlZVldzwrKwsGg6Ha9xgMhju2r/o1KysLAQEBdm1CQ0Ntbf53oHRFRQVyc3Nt71+0aBH0ej3mz59va7Nq1SoEBQUhOTkZffr0uaU2rVYLrVZbk0uXTFFZVehRSVwJyc1zg9tDqVRgwU/H8c6/T8AqgOcHt5e6LCIi2apVT49Go0FYWBgSExNtx6xWKxITExEVFVXte6KiouzaA8C2bdts7YODg2EwGOzamM1mJCcn29pERUUhLy8PaWlptjbbt2+H1WpFZGQkAKCoqAhKpf3lqFQqW42OqmpMj6uG43noVtN+1w4vDAkBAPxz2wm8958TEldERCRftX68NWvWLCxduhQrVqzA0aNHMXXqVBQWFiI+Ph4AMHbsWMyZM8fWfsaMGdi6dSveffddHDt2DK+99hpSU1Mxffp0AJWDM2fOnIk33ngD33//PQ4dOoSxY8ciMDAQcXFxAIBOnTphyJAhmDRpEvbu3Ytdu3Zh+vTpGDlyJAIDAwEAsbGxSElJweuvv46TJ09i3759iI+PR6tWrdCzZ8/7vU+Sqerpcdewp4eqN/WBtpjzUGXwee8/J/H+f/ioi4ioOrXuPhgxYgSuXr2KuXPnwmg0IjQ0FFu3brUNRL5w4YJdj0vfvn2xZs0avPLKK3jppZfQvn17bNy4EV27drW1mT17NgoLCzF58mTk5eWhf//+2Lp1K3Q6na3N6tWrMX36dAwePBhKpRLDhw/HwoULbecffPBBrFmzBvPnz8f8+fPh5uaGqKgobN26Fa6urvd0c+SgqKyyp8eNqzHTHUwZ1BZKhQJvbj6Kf/3nBLQuSjwzqK3UZRERyYpC3G6UrxMym83Q6/UwmUzw8vKSuhwAwBd7zuPVjRkY0sWAxWPCpC6HZG7Rz6ew4KfjAIB5Qzsjvl+wxBUREdW/mn5/N5rZW41V0Y3ZW24cyEw1MO137WyDmf/2wxGsTj4vcUVERPLB0CNzVY+33DmQmWroT9HtbVtWvPxtBr5KuyRxRURE8sDQI3NVA5nZ00M1pVAo8OKQEIzv2xoAMPurA/huf6a0RRERyQBDj8wVVg1kdmFPD9WcQqHAvKGd8WRkS1gFMOvLA9iacUXqsoiIJMXQI3NVY3rc2dNDtaRQKPDGsK54LKwFLFaB59amI/Fo1t3fSETUSDH0yJxtyjrH9NA9UCoVeHt4dwztEYhyi8DUVfvw35NXpS6LiEgSDD0y91voYU8P3RuVUoF/PtEDQ7oYUGaxYvLKNKSdvy51WUREDY6hR+YKqwYyM/TQfXBRKbFwVE8M7NAMxeUWxC/bi6NXzFKXRUTUoBh6ZK64aso6V2Sm+6RRK7H4qV4Ia9UE5pIKjPlsL87lFEpdFhFRg2Hokbmqnh5X9vRQHXDTqPH5uN4IMXgip6AUoz9NhtFUInVZREQNgqFH5opKuTgh1S29mwu+mBCJ1k3dkJlXjKc+S0ZuYZnUZRER1TuGHpnjmB6qD808tVg1MRIGLx1OZRcgftleFNxYHoGIqLFi6JExi1WgpNwKgGN6qO61aOKGVRMj4OOuwYFLJkxakYqScovUZRER1RuGHhkrvukLiD09VB/a+XliRXwEPLRqJJ25hulr0lFusUpdFhFRvWDokbGq1ZiVCkCr5h8V1Y9uLfT4dFw4tGol/nM0C7O/OgirVUhdFhFRneM3qYzdvMO6QqGQuBpqzPq0aYqPRveCWqnAt+mZ+Mfmo1KXRERU5xh6ZIzT1akhDe7kjwWPdwcAfLrzLJbsOC1xRUREdYuhR8aKuDAhNbBHerbASw+HAAD+sfkYvtl3SeKKiIjqDkOPjHHfLZLC5IFtMbF/MABg9lcH8cvxbIkrIiKqGww9MlY1kJmhhxraSw93QlxoICqsAs+u3ocDF/OkLomI6L4x9MhYoa2nh4+3qGEplQrMf6wHBrT3RVGZBfHLU3DmaoHUZRER3ReGHhkrujGQ2V3Lnh5qeBq1Eh8/FYZuzfXILSzD2M/3ItvMfbqIyHEx9MhYEXt6SGIeWjWWxfdG66ZuuHS9GOOWpcBcUi51WURE94ShR8Y4pofkwNdDi5VPR8LXQ4ujV8yYvDIVpRXcroKIHA9Dj4xxTA/JRcumblge3xseWjX2nMnFrPUHYOGqzUTkYBh6ZOy3FZnZ00PS69pcjyVjwuCiUuDHQ1e4ajMRORyGHhkr4orMJDN92/nincd7AAA+23kWn+88K3FFREQ1x9AjY4WlXJGZ5GdYaHO8MKRy1ea//3gEWzOMEldERFQzDD0yVtXTw4HMJDfPDGqD0ZEtIQQwY1069l24LnVJRER3xdAjYzfvsk4kJwqFAn/7Yxc8GOKH0gorJq5IxbmcQqnLIiK6I4YeGWNPD8mZWqXEB6N62hYvHL9sL3ILy6Qui4jothh6ZKxqTI8bx/SQTLlr1fhsfDhaNHHFuWtFmLgiBSXlXMOHiOSJoUfGiss5ZZ3kz89Th+XxvaF3dcG+C3mYuW4/1/AhIlli6JGxwlJOWSfH0M7PE0vGhEGjUmLrYSPX8CEiWWLokakKixWlFVYAHMhMjiGyTVO88wTX8CEi+WLokamim8ZFuHGXdXIQf+wRiBcf4ho+RCRPDD0yVXxjurpKqYBGxT8mchxTBrbBU31+W8MnnWv4EJFM8NtUpgpv2mFdoVBIXA1RzSkUCrw29Lc1fCatTMOl60VSl0VExNAjV1yYkByZWqXEwlE90SnACzkFpZiwPBX5JeVSl0VETo6hR6aqQg/H85Cj8tCq8dm4cPh5anE8Kx/T16SjwmKVuiwicmIMPTJVyNWYqREI9HbFZ+N6w9VFhV9PXMXffjgCIbiGDxFJg6FHpoqqVmPm4y1ycN1a6PHeyFAoFMAXe85j2a5zUpdERE6KoUemqnp6uBozNQYxXQyYc9NU9v8cyZK4IiJyRgw9MlU1ZZ09PdRYTBrQBqMigiAE8Py6dBy+bJK6JCJyMgw9MsUxPdTYKBQKvD6sK/q1a4qiMgsmLE9FlrlE6rKIyIkw9MhU1Zged+6wTo2Ii0qJj0aHoZ2fB4zmEkxYkYKiGwGfiKi+MfTIlG3KOnt6qJHRu7pg2fjeaOquQUamGTO4KzsRNRCGHpkq4uMtasSCfNywZGwYNGolth3Jwttbj0ldEhE5AYYemSrkQGZq5MJa+WDBY90BAEt2nMGa5AsSV0REjR1Dj0wV3dh7y50rMlMjNiy0OWb9XwcAwNzvMrD7dI7EFRFRY8bQI1NVY3pc2dNDjdxzD7ZDXGggKqwCU1ftw7mcQqlLIqJGiqFHpoq4OCE5CYVCgbeGd0dokDdMxeWYsCIFpmJuTkpEdY+hR6Y4poecic5FhSVjwxCg1+H01UI8t5abkxJR3WPokaliTlknJ+PnqcPSseFwdVFhx4mreHPzUalLIqJGhqFHpmx7b3EgMzmRrs31+NeIHgCAZbvOcUYXEdUphh6Z4i7r5KyGdA3Anzmji4jqwT2FnkWLFqF169bQ6XSIjIzE3r1779h+w4YNCAkJgU6nQ7du3bB582a780IIzJ07FwEBAXB1dUV0dDROnjxp1yY3NxejR4+Gl5cXvL29MWHCBBQUFNzyc9555x106NABWq0WzZs3x5tvvnkvlyipcosVZTfGM7gz9JATmv5gO/yxR+WMrmdXc0YXEdWNWoee9evXY9asWZg3bx727duHHj16ICYmBtnZ2dW23717N0aNGoUJEyYgPT0dcXFxiIuLQ0ZGhq3N/PnzsXDhQixevBjJyclwd3dHTEwMSkp+24xw9OjROHz4MLZt24ZNmzZhx44dmDx5st1nzZgxA59++ineeecdHDt2DN9//z0iIiJqe4mSq5quDgCuHNNDTkihUGD+Y93Ro4UeeUXlmLgyFeYSzugiovskaikiIkJMmzbN9nuLxSICAwNFQkJCte2feOIJERsba3csMjJSTJkyRQghhNVqFQaDQSxYsMB2Pi8vT2i1WrF27VohhBBHjhwRAERKSoqtzZYtW4RCoRCZmZm2Nmq1Whw7dqy2l2RjMpkEAGEyme75Z9SFy3lFotULm0S7l36UtA4iqWWZikXkm/8RrV7YJMZ+lizKKyxSl0REMlTT7+9a9fSUlZUhLS0N0dHRtmNKpRLR0dFISkqq9j1JSUl27QEgJibG1v7s2bMwGo12bfR6PSIjI21tkpKS4O3tjfDwcFub6OhoKJVKJCcnAwB++OEHtGnTBps2bUJwcDBat26NiRMnIjc397bXU1paCrPZbPeSg0KO5yECAPh56fDpuHDoXJT49cRV/GMz9+giontXq9CTk5MDi8UCf39/u+P+/v4wGo3VvsdoNN6xfdWvd2vj5+dnd16tVsPHx8fW5syZMzh//jw2bNiAlStXYvny5UhLS8Njjz122+tJSEiAXq+3vYKCgu52CxoEp6sT/aZrcz3++UQoAODzXWexbi9ndBHRvWk0s7esVitKS0uxcuVKDBgwAA888AA+++wz/Pzzzzh+/Hi175kzZw5MJpPtdfHixQauunqF3GGdyM7D3QLwp+jKGV2vbMzAnjPXJK6IiBxRrUKPr68vVCoVsrKy7I5nZWXBYDBU+x6DwXDH9lW/3q3N/w6UrqioQG5urq1NQEAA1Go1OnToYGvTqVMnAMCFC9X/y1Cr1cLLy8vuJQe2LSi0fLxFVOX5we3wh+4BN/boSsOFa0VSl0REDqZWoUej0SAsLAyJiYm2Y1arFYmJiYiKiqr2PVFRUXbtAWDbtm229sHBwTAYDHZtzGYzkpOTbW2ioqKQl5eHtLQ0W5vt27fDarUiMjISANCvXz9UVFTg9OnTtjYnTpwAALRq1ao2lyk522ajLuzpIaqiUCjwzuM90L2FHteLKvfoyueMLiKqhVo/3po1axaWLl2KFStW4OjRo5g6dSoKCwsRHx8PABg7dizmzJljaz9jxgxs3boV7777Lo4dO4bXXnsNqampmD59OoDKv8hmzpyJN954A99//z0OHTqEsWPHIjAwEHFxcQAqe2yGDBmCSZMmYe/evdi1axemT5+OkSNHIjAwEEDlwOZevXrh6aefRnp6OtLS0jBlyhT83//9n13vjyOoWpiQPT1E9nQuKiwdGw5/Ly1OZhfgubXpsFiF1GURkYOodegZMWIE3nnnHcydOxehoaHYv38/tm7dahuIfOHCBVy5csXWvm/fvlizZg2WLFmCHj164KuvvsLGjRvRtWtXW5vZs2fjueeew+TJk9G7d28UFBRg69at0Ol0tjarV69GSEgIBg8ejIcffhj9+/fHkiVLfrsQpRI//PADfH19MXDgQMTGxqJTp05Yt27dPd0YKXFMD9Ht+XtV7tGlc1Hil+NXMX8rZ3QRUc0ohBD8Z9INZrMZer0eJpNJ0vE9i34+hQU/HceI8CC8/Vh3yeogkrMfDlzGc2vTAQD/GtEDj/RsIXFFRCSVmn5/N5rZW41J1UBmrsZMdHtDewTi2QfaAgBe+PoQDlzMk7YgIpI9hh4ZKrSN6WHoIbqTv/y+I6I7+aGsworJX6Qi21xy9zcRkdNi6JGhItuYHg5kJroTpVKBf40IRXs/D2SZSzH5izSUlFvu/kYickoMPTJUxBWZiWrMU+eCpWPDoXd1wf6LeXj52wxwqCIRVYehR4aqQo87e3qIaqS1rzs+fLInlArg632X8NnOs1KXREQyxNAjQ4WlNx5vcUwPUY0NaN8Mr8R2BgD8Y/NR/HriqsQVEZHcMPTIEHt6iO5NfL/WeDysBawCeG7NPpzNKZS6JCKSEYYeGeKUdaJ7o1Ao8MYjXdGrpTfMJRWYuCIFZm5VQUQ3MPTIEHt6iO6dVq3C4jFhCNDrcPpqIWau28+tKogIAEOPLHFMD9H98fPUYcmYcGjVSmw/lo0FPx2XuiQikgGGHhkqLueUdaL71a2FHvNvbOOy+NfT+G5/psQVEZHUGHpkpqzCinJLZVc8Fyckuj/DQpvjmUGVW1XM/uogDl7Kk7YgIpIUQ4/MVA1iBtjTQ1QX/hrTEQ+G+KG0worJK9O4VQWRE2PokZnCG4OYNSolXFT84yG6XyqlAu+NDEXbZu4wmkvwzKo0lFZwqwoiZ8RvVZkpLuMgZqK65qVzwafjesNLp8a+C3l4hVtVEDklhh6Zse2wzvE8RHUq2NcdHz7ZC0oFsCHtEpbtOid1SUTUwBh6ZKbQtsM6e3qI6trADs3w0sOdAABv/HgE/z3JrSqInAlDj8wUc4d1ono1oX8whveq3Kpi+pp0nONWFUROg6FHZgptoYePt4jqg0KhwJuPdEVokDdMxeWYuDIV+dyqgsgpMPTITNGN1ZjdOZCZqN7oXFRYMiYM/l5anMou4FYVRE6CoUdmqnp6XNnTQ1Sv/Lwqt6rQqJVIPJaNd//NrSqIGjuGHpmpmrLuzjE9RPWuR5A35g+v3Krio19O4/sDlyWuiIjqE0OPzHBMD1HDiuvZHFMGtgEAzP7qADIyTRJXRET1haFHZqrG9HD2FlHDmT0kBA90bIaScismrUzF1fxSqUsionrA0CMzRVU9PRzITNRgVEoF3h/ZE22aueOKqQRTuVUFUaPE0CMzVaGHKzITNSy9qwuWjg2Hp06N1PPXMe+7w9yqgqiRYeiRGa7ITCSdts08sHBUTygUwLqUi1iZdF7qkoioDjH0yEwRBzITSep3Hf3w4pAQAMDrm45g96kciSsiorrC0CMzRdxlnUhykwe2wSM9m8NiFXh2zT5cuFYkdUlEVAcYemSmiLusE0lOoVAg4dFu6N5Cj7yickxamYqCGzMrichxMfTIDMf0EMlD5VYV4WjmqcXxrHzMWr8fVm5VQeTQGHpkpoi7rBPJhkGvwydjwqBRKfHvI1l4L/Gk1CUR0X1g6JERIcRvU9a1fLxFJAe9WjbBm490BQAsTDyJLYeuSFwREd0rhh4ZKa2w2nZ6Zk8PkXw8Hh6Ep/sFAwBmfXkARy6bJa6IiO4FQ4+MFJf9tgIsp6wTyctLD4dgQHtfFJdbMGllKq4VcKsKIkfD0CMjVYOYtWolVEqFxNUQ0c3UKiU+GNUTrZq6ITOvGM+u3odyi1XqsoioFhh6ZITjeYjkzdtNg0/HhsNDq0by2Vz87YfDUpdERLXA0CMjhTfWAXF14XgeIrlq7++J90aEQqEAVu25gNXJ3KqCyFEw9MhIsa2nh6GHSM6iO/vjL7/vCACY991hJJ+5JnFFRFQTDD0yUsh9t4gcxrMPtMXQHoGosApMXb0Pl65zqwoiuWPokZEirsZM5DAUCgXmD++Ors29kFtYhkkr02z/DRORPDH0yAh3WCdyLK6ayq0qfD00OHrFjL9uOAghuFUFkVwx9MhI1UBmjukhchyB3q5Y/FQYXFQK/HjoCj7cfkrqkojoNhh6ZIQ9PUSOKby1D/4+rHKrine3ncC/DxslroiIqsPQIyPcYZ3IcY2MaIlxUa0AAH9avx/HjfkSV0RE/4uhR0ZsU9YZeogc0it/6IyoNk1RWGbBxJUpuF5YJnVJRHQThh4ZKSy98XiLKzITOSQXlRIfje6FIB9XXMwtxrQ13KqCSE4YemSEU9aJHF8Tdw2Wjg2Hm0aF3aev4c0fj0pdEhHdwNAjIxzITNQ4hBi88M8nQgEAy3efw/qUC9IWREQAGHpkpaqnh2N6iBzfkK4G/Cm6AwDglY0ZSD2XK3FFRMTQIyMc00PUuDz3YDs81NWAcovAM6vScDmvWOqSiJwaQ4+McEwPUeOiVCrwzuM9EGLwRE5BGSZ/kWqbpUlEDY+hR0Z+G9PD0EPUWLhr1Vg6Nhw+7hpkZJox+2tuVUEkFYYeGSmyrdPDx1tEjUmQjxs+Gt0LaqUCPxy4jI9/PS11SUROiaFHJoQQXJGZqBHr06Yp5v2xCwBgwU/HkXg0S+KKiJwPQ49MlFZYUdXjzYHMRI3TmD6t8GRkSwgBzFi3H6eyuVUFUUO6p9CzaNEitG7dGjqdDpGRkdi7d+8d22/YsAEhISHQ6XTo1q0bNm/ebHdeCIG5c+ciICAArq6uiI6OxsmTJ+3a5ObmYvTo0fDy8oK3tzcmTJiAgoKCaj/v1KlT8PT0hLe3971cniSqdlgHAFcX9vQQNVavDe2CiNY+KCitwMQVqTAVlUtdEpHTqHXoWb9+PWbNmoV58+Zh37596NGjB2JiYpCdnV1t+927d2PUqFGYMGEC0tPTERcXh7i4OGRkZNjazJ8/HwsXLsTixYuRnJwMd3d3xMTEoKSkxNZm9OjROHz4MLZt24ZNmzZhx44dmDx58i2fV15ejlGjRmHAgAG1vTRJVY3n0bkooVIqJK6GiOqLRq3ER0/1QnNvV5y7VoTpa/ehgltVEDUMUUsRERFi2rRptt9bLBYRGBgoEhISqm3/xBNPiNjYWLtjkZGRYsqUKUIIIaxWqzAYDGLBggW283l5eUKr1Yq1a9cKIYQ4cuSIACBSUlJsbbZs2SIUCoXIzMy0+9mzZ88WTz31lFi2bJnQ6/W1ujaTySQACJPJVKv31YWjV0yi1QubRK/X/93gn01EDS8jM0+EvLJFtHphk3jt+wypyyFyaDX9/q5VT09ZWRnS0tIQHR1tO6ZUKhEdHY2kpKRq35OUlGTXHgBiYmJs7c+ePQuj0WjXRq/XIzIy0tYmKSkJ3t7eCA8Pt7WJjo6GUqlEcnKy7dj27duxYcMGLFq0qEbXU1paCrPZbPeSim26upaPtoicQZdAPf75RA8AwLJd57A6+bzEFRE1frUKPTk5ObBYLPD397c77u/vD6PRWO17jEbjHdtX/Xq3Nn5+fnbn1Wo1fHx8bG2uXbuG8ePHY/ny5fDy8qrR9SQkJECv19teQUFBNXpffSgq5XR1ImfzULcA/OX3lVtVzPvuMHafypG4IqLGrdHM3po0aRKefPJJDBw4sMbvmTNnDkwmk+118eLFeqzwzqqmq7tyujqRU5n2u3YYFhqICqvA1NX7cOZq9RM0iOj+1Sr0+Pr6QqVSISvLfn2JrKwsGAyGat9jMBju2L7q17u1+d+B0hUVFcjNzbW12b59O9555x2o1Wqo1WpMmDABJpMJarUan3/+ebW1abVaeHl52b2kUsyFCYmckkKhwNvDuyM0yBum4nLO6CKqR7UKPRqNBmFhYUhMTLQds1qtSExMRFRUVLXviYqKsmsPANu2bbO1Dw4OhsFgsGtjNpuRnJxsaxMVFYW8vDykpaXZ2mzfvh1WqxWRkZEAKsf97N+/3/Z6/fXX4enpif379+ORRx6pzWVKggsTEjkvnYsKS8aGIVCvw5mcQjy7Jg3lnNFFVOdq3a0wa9YsjBs3DuHh4YiIiMB7772HwsJCxMfHAwDGjh2L5s2bIyEhAQAwY8YMDBo0CO+++y5iY2Oxbt06pKamYsmSJQAq/5Uzc+ZMvPHGG2jfvj2Cg4Px6quvIjAwEHFxcQCATp06YciQIZg0aRIWL16M8vJyTJ8+HSNHjkRgYKCtzc1SU1OhVCrRtWvXe745DalqTA9DD5Fz8vPU4dNxvfHY4t3YdeoaXv/hCP4e5xh/fxE5ilqHnhEjRuDq1auYO3cujEYjQkNDsXXrVttA5AsXLkCp/K0DqW/fvlizZg1eeeUVvPTSS2jfvj02btxoF0Zmz56NwsJCTJ48GXl5eejfvz+2bt0KnU5na7N69WpMnz4dgwcPhlKpxPDhw7Fw4cL7uXZZ+W1MDx9vETmrzoFeeH9kT0z+IhVf7DmPdn4eGNe3tdRlETUaCiG43W8Vs9kMvV4Pk8nU4ON7/rH5KJbsOINJA4LxcmznBv1sIpKXxb+exltbjkGpAJbHR2Bgh2ZSl0QkazX9/m40s7ccXdU2FG7s6SFyelMGtsHwXi1gFcC0Nfu4RxdRHWHokYmqxQk9uNkokdNTKBT4x6Nd0bt1E+SXVGDCilRcLyyTuiwih8fQIxO2nh6uyExEALRqFRY/FYYWTVxx/loRnlmVhrIKzugiuh8MPTJRNZCZ6/QQUZWmHlp8Nq43PLRqJJ/NxasbM8BhmET3jqFHJgo5ZZ2IqtHR4IkPRvWEUgGsT72Iz3aelbokIofF0CMTRTd6ejimh4j+1+9C/PDSw5Vrkf1j81FsP5Z1l3cQUXUYemTC1tPD0ENE1ZjQPxijIoJgFcDza/fjmNEsdUlEDoehRyZ+G9PDx1tEdCuFQoG//bEr+rTxQUFpBSYsT0W2uUTqsogcCkOPTBSxp4eI7kKjVmLxU2Fo4+uOzLxiTFyZans0TkR3x9AjA2UVVpTd2FzQg7O3iOgOvN00WBbfGz7uGhy8ZMKMdfthsXJGF1FNMPTIQPGNhQkBwJWPt4joLlo1dceSMWHQqJTYdiQLCZuPSl0SkUNg6JGBghvd0xqVEho1/0iI6O7CW/tgwePdAQCf7jyLL/acl7giIvnjN6wMFHE1ZiK6B8NCm+Mvv+8AAJj3XQZ+Pp4tcUVE8sbQIwOFNx5vcTVmIqqtab9rh8fCKjcnnb56H45c5lR2otth6JGBqp4ed/b0EFEtKRQK/OORbohq0xSFZRZMWJGCLE5lJ6oWQ48MFFQ93mJPDxHdg6qp7G2bueOKqQQTVqRwKjtRNRh6ZKCo6vEWe3qI6B7p3VywbHwEmrprkJFpxvNrOZWd6H8x9MgAd1gnorrQsqkblowNh0atxH+OZuHNHzmVnehmDD0yULUasztXYyai+xTWqgn++UQPAMDnu85iZdI5aQsikhGGHhn4bUwPH28R0f37Q/dAzB7SEQDw2veHuSs70Q0MPTJQNeCQPT1EVFemDmqLEeGVu7JPW52OAxfzpC6JSHIMPTLAdXqIqK4pFAq88UhXDGjvi+JyC55enoLz1wqlLotIUgw9MsB1eoioPriolPj4qTB0CfTCtcIyjF+WgmsFpVKXRSQZhh4ZKLgxkJnr9BBRXfPQqrFsfG8093bF2ZxCTFyZarfJMZEzYeiRgd/G9LCnh4jqnp+XDiue7g29qwvSL+Th+XXpXMOHnBJDjwxwTA8R1bd2fp74dFzlGj7bjmRh3vcZEILBh5wLQ48McJd1ImoIvVv74P0RoVAogFV7LuDjX09LXRJRg2LokYHCUq7ITEQN46FuAZj7h84AgPlbj+ObfZckroio4TD0yEAh994iogYU3y8YkwYEAwBmf3UQO0/mSFwRUcNg6JEBLk5IRA1tzkOdMLRHICqsAs+sSsORy2apSyKqdww9EiursKLcUjmYkFPWiaihKJUKvPN4d0QG+6CgtALjl+3FpetFUpdFVK8YeiRWNZ4H4N5bRNSwtGoVlowNRwd/D2Tnl2Lc53uRW1gmdVlE9YahR2KFNx5tadRKuKj4x0FEDUvv6oLl8REI0Otw+moh4pen2P1jjKgx4besxIpuDGL24HgeIpJIoLcrvpgQAW83Fxy4mIdnVqWhrMIqdVlEdY6hR2JV/6Lioy0iklI7P08sG98bri4q/PdkDv684QCsXLWZGhmGHokVlnI1ZiKSh54tm2DxmDColQr8cOAy/vbDYa7aTI0KQ4/Eqsb0cDVmIpKDQR2a4d0negAAViSdx4fbT0lcEVHdYeiRWEFJZejx1LlIXAkRUaVhoc0xb2jlqs3vbjuB1cnnJa6IqG4w9Eis4MaYHk8OZCYiGYnvF4znHmwHAHhlYwY2H7oicUVE94+hR2JVoYezt4hIbmb9XweMimgJIYCZ6/Zj9yluV0GOjaFHYvk3Hm956Bh6iEheFAoF3ojrioe6GlBmsWLyF2k4dMkkdVlE94yhR2IFpeUA2NNDRPKkUirw3shQ9G3bFAWlFRj7eTJOZuVLXRbRPWHokVjVQGaGHiKSK61ahU/GhKFHCz2uF5Vj9KfJOH+tUOqyiGqNoUditjE9fLxFRDLmqavcrqKjvyey80sx+tNkXDEVS10WUa0w9Egsnz09ROQgmrhr8MXECAT7uuPS9WKM/jQZOQWlUpdFVGMMPRJjTw8RORI/Tx1WTYxEoF6HM1cLMeazvTAVlUtdFlGNMPRIjOv0EJGjae7titWT+sDXQ4ujV8wYv3wvd2Ynh8DQI7ECTlknIgcU7OuOVRMrd2ZPv5CHSStTUVJukbosojti6JFYPhcnJCIHFWLwwor4CLhrVNh9+hqmrd6HcotV6rKIbouhR0KlFRaUVVT+BeGp5d5bROR4egR547PxvaFVK5F4LBsz1++Hxcqd2UmeGHokVFj6W1ewO3dZJyIH1adNU3wyJgwuKgV+PHgFf9lwgMGHZImhR0JV43lcXVRQq/hHQUSO64GOfvhgVC+olAp8m56J2V8dhJXBh2SG37QSyq/agoKDmImoERjS1YAPRvWESqnA1/suYc43hxh8SFYYeiRU1dPD6epE1Fg83C0A740IhVIBrE+9iJc3ZjD4kGww9EiICxMSUWM0tEcg/nUj+KzdewFzv8+AEAw+JD2GHgkVcLo6ETVSw0Kb453He0ChAFbtuYDXvj/M4EOSu6fQs2jRIrRu3Ro6nQ6RkZHYu3fvHdtv2LABISEh0Ol06NatGzZv3mx3XgiBuXPnIiAgAK6uroiOjsbJkyft2uTm5mL06NHw8vKCt7c3JkyYgIKCAtv5X375BcOGDUNAQADc3d0RGhqK1atX38vlNRjuu0VEjdmjvVpg/vDuUCiAFUnn8fqmIww+JKlah57169dj1qxZmDdvHvbt24cePXogJiYG2dnZ1bbfvXs3Ro0ahQkTJiA9PR1xcXGIi4tDRkaGrc38+fOxcOFCLF68GMnJyXB3d0dMTAxKSkpsbUaPHo3Dhw9j27Zt2LRpE3bs2IHJkyfbfU737t3x9ddf4+DBg4iPj8fYsWOxadOm2l5ig+HjLSJq7B4PD8Jbj3YDACzbdQ7/2HyUwYekI2opIiJCTJs2zfZ7i8UiAgMDRUJCQrXtn3jiCREbG2t3LDIyUkyZMkUIIYTVahUGg0EsWLDAdj4vL09otVqxdu1aIYQQR44cEQBESkqKrc2WLVuEQqEQmZmZt6314YcfFvHx8TW+NpPJJAAIk8lU4/fcjwVbj4lWL2wSczceapDPIyKSyuo950WrFzaJVi9sEq//cFhYrVapS6JGpKbf37Xq6SkrK0NaWhqio6Ntx5RKJaKjo5GUlFTte5KSkuzaA0BMTIyt/dmzZ2E0Gu3a6PV6REZG2tokJSXB29sb4eHhtjbR0dFQKpVITk6+bb0mkwk+Pj63PV9aWgqz2Wz3akjs6SEiZ/FkZEu8EdcVAPDZzrOY9/1hzuqiBler0JOTkwOLxQJ/f3+74/7+/jAajdW+x2g03rF91a93a+Pn52d3Xq1Ww8fH57af++WXXyIlJQXx8fG3vZ6EhATo9XrbKygo6LZt68NvY3q4BQURNX5P9WmFt4d3g0IBrEw6j5e+5To+1LAa5eytn3/+GfHx8Vi6dCm6dOly23Zz5syByWSyvS5evNiAVQL5JVyckIicy4jeLfHu4z2gVADrUi7iL19xywpqOLUKPb6+vlCpVMjKyrI7npWVBYPBUO17DAbDHdtX/Xq3Nv87ULqiogK5ubm3fO6vv/6KoUOH4l//+hfGjh17x+vRarXw8vKyezUkU3Fl6NG7sqeHiJzHo71a4P2RlSs3f7MvEzPX7+fu7NQgahV6NBoNwsLCkJiYaDtmtVqRmJiIqKioat8TFRVl1x4Atm3bZmsfHBwMg8Fg18ZsNiM5OdnWJioqCnl5eUhLS7O12b59O6xWKyIjI23HfvnlF8TGxuLtt9+2m9klV+Ybj7cYeojI2QztEYhFT/aCi0qBHw5cxnNr0lFWweBD9avWj7dmzZqFpUuXYsWKFTh69CimTp2KwsJC29iZsWPHYs6cObb2M2bMwNatW/Huu+/i2LFjeO2115Camorp06cDABQKBWbOnIk33ngD33//PQ4dOoSxY8ciMDAQcXFxAIBOnTphyJAhmDRpEvbu3Ytdu3Zh+vTpGDlyJAIDAwFUPtKKjY3F888/j+HDh8NoNMJoNCI3N/d+71G9Md/o6fHi4y0ickJDuhqw+KkwaFRKbD1sxNRVaSgpt0hdFjVm9zI17IMPPhAtW7YUGo1GREREiD179tjODRo0SIwbN86u/Zdffik6dOggNBqN6NKli/jxxx/tzlutVvHqq68Kf39/odVqxeDBg8Xx48ft2ly7dk2MGjVKeHh4CC8vLxEfHy/y8/Nt58eNGycA3PIaNGhQja+roaesd5m7VbR6YZM4nZ1/98ZERI3Ur8ezRYeXN4tWL2wST326RxSWlktdEjmYmn5/K4TgKlFVzGYz9Ho9TCZTvY/vqbBY0e7lLQCAtFei0dRDW6+fR0QkZ7tP52DC8lQUl1sQ1qoJPh/XG3o3Pvqnmqnp93ejnL3lCKqmqwOAF8f0EJGT69vWF6smRsJLp0ba+esYsSQJ2fkld38jUS0w9EjEfGO6uptGBRcV/xiIiMJaNcGXz0ShmacWx4z5eHxxEi7mFkldFjUi/LaVCKerExHdKsTgha+eiUKQjyvOXyvCY4t340RWvtRlUSPB0CMRk23mFkMPEdHNWjV1x1fP9EVHf09kmUvxxCdJ2H8xT+qyqBFg6JGIuZhr9BAR3Y6/lw7rp/RBaJA38orK8eTSPdh1KkfqssjBMfRIxNbTw9BDRFQtbzcNVk+MRP92vigqsyB+WQo2HbwsdVnkwBh6JPJb6OHChEREt+OuVeOz8eF4uJsBZRYrpq9Jx6f/PSN1WeSgGHokUjV7i4+3iIjuTKtW4YNRvTC+b2sAwBs/HsXrPxzhDu1Uaww9EuFAZiKimlMpFZg3tDPmPBQCAPh811k8ty6d21ZQrTD0SMTMKetERLWiUCgwZVBbvD8yFC4qBX48eAVjP98LU1G51KWRg2DokQjX6SEiujfDQptjRXwEPLVq7D2bi8cW78blvGKpyyIHwNAjETNnbxER3bO+7Xzx5TNR8PfS4mR2AeIW7cKhSyapyyKZY+iRiLmE6/QQEd2PTgFe+ObZfujg74Hs/FI8/slubM24InVZJGMMPRLh4y0iovvX3NsVX03ti0EdmqGk3IpnVu3Dop9PQQjO7KJbMfRIQAhx0+MtrtNDRHQ/vHQu+GxcuG1K+4KfjuMvGw6itIIzu8geQ48ECkorUHFjfQlvV43E1RAROT61SonX/tgFfx/WBSqlAl/vu4Qxn+5FbmGZ1KWRjDD0SKDqP0JXFxVcNSqJqyEiajzGRLXG5+N7V87sOpeLuEW7cCqbu7RTJYYeCVSFHh939vIQEdW1QR2a4Ztn+yLIxxUXcosQt2g3th3JkroskgGGHgkw9BAR1a/2/p7Y+Gw/RAb7oKC0ApNWpuJf205w6wonx9AjAYYeIqL619RDi1UTI20DnN9PPInJX6TZ9j4k58PQIwGGHiKihuFyY4DzO4/3gEatxH+OZt0Y51MgdWkkAYYeCeQWVYaeJm4MPUREDeGxsBb46pkoBOh1OHO1EHGLduHfh41Sl0UNjKFHArkFlaGnqQdDDxFRQ+newhs/PNcfETfG+Uz+Ig0Jm4+i3GKVujRqIAw9Eqh6vMWeHiKihuXrocXqiZF4ul8wAOCTHWcwaskeXDFxw1JnwNAjgarHWxzTQ0TU8FxUSswd2hkfj+4FT60aqeevI3bhTvxyPFvq0qieMfRIgAOZiYik91C3AGx6vj+6BHoht7AM45el4J2fjqOCj7saLYYeCVwrYOghIpKDVk3d8fXUvniqT0sAwIc/n8LoT5NhNJVIXBnVB4aeBlZYWoGC0goAgL+XVuJqiIhI56LCG3HdsHBUT7hrVEg+m4sh7+/A1owrUpdGdYyhp4Fl55cCANw0KnhoucM6EZFc/LFHIH54rj+6Ndcjr6gcz6zahxe+OojCG/9QJcfH0NPAssyVXab+XjooFAqJqyEiopu1aeaBr6f2xdQH2kKhANanXkTswv/iwMU8qUujOsDQ08CqQo+fJx9tERHJkUatxAtDQrBmYh8E6nU4d60Iwz/ejQ+3n4SFe3c5NIaeBnb1xuMtPy+dxJUQEdGdRLVtii0zBiK2ewAqrALv/PsEHl+8G6evcgsLR8XQ08Bsj7fY00NEJHt6Nxd8OKon3n28Bzy0auy7kIeH3/8vluw4zV4fB8TQ08CyzJU9Pf7s6SEicggKhQLDw1rgpz8NxID2viitsOIfm4/hscW7uXGpg2HoaWDZ+TfG9HC6OhGRQ2nu7YqVT0fg7eHd4KlVI/1CHh5e+F988utpLmjoIBh6GtjlvMrQY2BPDxGRw1EoFBjRuyV++tNADOrQDGUVViRsOYa4j3bh4KU8qcuju2DoaUDlFisy8yo3tWvV1F3iaoiI6F4FertieXxvzB/eHV46NTIyzRi2aBfmfZcBc0m51OXRbTD0NKDM68WwWAV0LkpOWScicnAKhQJP9A5C4p8fQFxoIIQAViSdR/S7v2LTwcsQggOd5YahpwGdzy0CALT0cYNSyYUJiYgag2aeWrw3sidWT4xEsK87svNLMX1NOsYvS8EZTm+XFYaeBnT+WiEAPtoiImqM+rXzxZYZAzBjcHtoVEr8euIqYt7bgTc2HYGpmI+85IChpwGdv1bZ09PKx03iSoiIqD7oXFT40/91wNaZA/BgiB/KLQKf7jyL373zC1btOc9ZXhJj6GlA53KqenoYeoiIGrM2zTzw+fjeWPF0BNr5eSC3sAyvbMzAHz7YiV2ncqQuz2kx9DSgI1fMAICOBi+JKyEiooYwqEMzbJkxAK8N7Qy9qwuOGfMx+tNkjPksGYcumaQuz+kw9DSQnIJSXDFVrtHTOZChh4jIWbiolBjfLxi//vUBjO/bGmqlAv89mYOhH+7EtNX7uJdXA2LoaSDJZ3IBAO39POChVUtcDRERNTRvNw1e+2MXbP/zA3ikZ3MoFMCPh67g9//agRe+OohL14ukLrHRY+hpIP8+YgQA9G/vK3ElREQkpZZN3fCvEaHYMmMAojv5wWIVWJ96EQ8s+AWzvzqAszfGf1LdY+hpAAsTT+K7/ZcBAHGhzSWuhoiI5CDE4IVPx/XG11Oj0K9dU1RYBb5MvYTB7/6C59em47gxX+oSGx2GngbgplEBAMb3bY0eQd7SFkNERLIS1soHqyf2wTfP9sXgED9YBfD9gcuIeW8HJq5IxZ4z17i6cx1RCN5JG7PZDL1eD5PJBC+vuhtsLIRA0plr6NuWj7aIiOjOMjJN+OiXU9iSYUTVN3SXQC883S8Yf+gRAK1aJW2BMlTT72+GnpvUV+ghIiKqrVPZBfh811l8s+8SSsorFzX09dDiqT4tMSqiJfy9dBJXKB8MPfeAoYeIiOTmemEZ1qZcwMrd52E0Vy59olIq8LuOfhjZOwgPdGwGtcq5R6sw9NwDhh4iIpKrcosVWzKMWLn7HFLPX7cd9/fS4vGwIDwW1gKtfZ1zb0eGnnvA0ENERI7gVHY+1qdcxNf7MpFbWGY73qOFHkN7BOIP3QNh0DvP4y+GnnvA0ENERI6krMKK/xzNwrqUi9h1KgcWa+VXukIB9G7tg6HdAxDd2R8BeleJK61fDD33gKGHiIgcVU5BKbYcuoLvD1xGyrnrdue6BHphcCd/RHfyQ9dAPZRKhURV1g+GnnvA0ENERI3B5bxibDp4GVsyjNh/MQ83f9M389SiX9um6NvWF1FtmyLIx026QutITb+/72m496JFi9C6dWvodDpERkZi7969d2y/YcMGhISEQKfToVu3bti8ebPdeSEE5s6di4CAALi6uiI6OhonT560a5Obm4vRo0fDy8sL3t7emDBhAgoK7DdpO3jwIAYMGACdToegoCDMnz//Xi6PiIjIoQV6u2LywLb49tl+SHk5Ggse644hXQxw06hwNb8UG/dfxuyvD2LA/J/R/+3t+MuGA1idfB4ZmSaUW6xSl19vat3Ts379eowdOxaLFy9GZGQk3nvvPWzYsAHHjx+Hn5/fLe13796NgQMHIiEhAX/4wx+wZs0avP3229i3bx+6du0KAHj77beRkJCAFStWIDg4GK+++ioOHTqEI0eOQKerHIj10EMP4cqVK/jkk09QXl6O+Ph49O7dG2vWrAFQmfI6dOiA6OhozJkzB4cOHcLTTz+N9957D5MnT67RtbGnh4iIGrPSCgvSzl1H0plrSDp9Dfsv5qHCah8DdC5KdA3Uo1sLPTr6e6K9vyc6+HvAU+ciUdV3V2+PtyIjI9G7d298+OGHAACr1YqgoCA899xzePHFF29pP2LECBQWFmLTpk22Y3369EFoaCgWL14MIQQCAwPx5z//GX/5y18AACaTCf7+/li+fDlGjhyJo0ePonPnzkhJSUF4eDgAYOvWrXj44Ydx6dIlBAYG4uOPP8bLL78Mo9EIjUYDAHjxxRexceNGHDt2rEbXxtBDRETOpLC0AqnnryPlbC4OXMrD/ot5yC+pqLZtgF6Hdn4eaNHEDS2auNpeBr0rmrproHORbqXomn5/q2vzQ8vKypCWloY5c+bYjimVSkRHRyMpKana9yQlJWHWrFl2x2JiYrBx40YAwNmzZ2E0GhEdHW07r9frERkZiaSkJIwcORJJSUnw9va2BR4AiI6OhlKpRHJyMh555BEkJSVh4MCBtsBT9Tlvv/02rl+/jiZNmtxSW2lpKUpLS22/N5vNtbkdREREDs1dq8agDs0wqEMzAIDVKnD2WiEOXMxDRqYZJ7PzcSIrH1nmUlwxleCKqeS2P8vVRQUfdw183DXwclVDp1ZB56KC1kUJnYsKmhsLKEZ38kf/9tJsy1Sr0JOTkwOLxQJ/f3+74/7+/rftTTEajdW2NxqNtvNVx+7U5n8fnanVavj4+Ni1CQ4OvuVnVJ2rLvQkJCTgb3/72+0vmIiIyIkolQq0beaBts088Giv346bispxMjsfZ3IKkXm9GJeuF+PS9SJcul6M7PwSlFsEisstyMwrRmZe8R0/w89L6xihp7GZM2eOXS+U2WxGUFCQhBURERHJj97NBeGtfRDe2ueWc0IIFJRWILewzPbKL6lASbkFpRVWlJRbUFJuRbnFCgGBXi1v7YRoKLUKPb6+vlCpVMjKyrI7npWVBYPBUO17DAbDHdtX/ZqVlYWAgAC7NqGhobY22dnZdj+joqICubm5dj+nus+5+TP+l1arhVarve31EhER0Z0pFAp46lzgqXNBq6by3gajVlPWNRoNwsLCkJiYaDtmtVqRmJiIqKioat8TFRVl1x4Atm3bZmsfHBwMg8Fg18ZsNiM5OdnWJioqCnl5eUhLS7O12b59O6xWKyIjI21tduzYgfLycrvP6dixY7WPtoiIiMjJiFpat26d0Gq1Yvny5eLIkSNi8uTJwtvbWxiNRiGEEGPGjBEvvviirf2uXbuEWq0W77zzjjh69KiYN2+ecHFxEYcOHbK1eeutt4S3t7f47rvvxMGDB8WwYcNEcHCwKC4utrUZMmSI6Nmzp0hOThY7d+4U7du3F6NGjbKdz8vLE/7+/mLMmDEiIyNDrFu3Tri5uYlPPvmkxtdmMpkEAGEymWp7W4iIiEgiNf3+rnXoEUKIDz74QLRs2VJoNBoREREh9uzZYzs3aNAgMW7cOLv2X375pejQoYPQaDSiS5cu4scff7Q7b7Vaxauvvir8/f2FVqsVgwcPFsePH7drc+3aNTFq1Cjh4eEhvLy8RHx8vMjPz7drc+DAAdG/f3+h1WpF8+bNxVtvvVWr62LoISIicjw1/f7mNhQ34To9REREjqdet6EgIiIicjQMPUREROQUGHqIiIjIKTD0EBERkVNg6CEiIiKnwNBDREREToGhh4iIiJwCQw8RERE5BYYeIiIicgq12mW9satanNpsNktcCREREdVU1ff23TaZYOi5SX5+PgAgKChI4kqIiIiotvLz86HX6297nntv3cRqteLy5cvw9PSEQqGo059tNpsRFBSEixcvcl+vesT73DB4nxsG73PD4b1uGPV1n4UQyM/PR2BgIJTK24/cYU/PTZRKJVq0aFGvn+Hl5cX/oBoA73PD4H1uGLzPDYf3umHUx32+Uw9PFQ5kJiIiIqfA0ENEREROgaGngWi1WsybNw9arVbqUho13ueGwfvcMHifGw7vdcOQ+j5zIDMRERE5Bfb0EBERkVNg6CEiIiKnwNBDREREToGhh4iIiJwCQ08DWLRoEVq3bg2dTofIyEjs3btX6pIcSkJCAnr37g1PT0/4+fkhLi4Ox48ft2tTUlKCadOmoWnTpvDw8MDw4cORlZVl1+bChQuIjY2Fm5sb/Pz88Ne//hUVFRUNeSkO5a233oJCocDMmTNtx3if60ZmZiaeeuopNG3aFK6urujWrRtSU1Nt54UQmDt3LgICAuDq6oro6GicPHnS7mfk5uZi9OjR8PLygre3NyZMmICCgoKGvhTZslgsePXVVxEcHAxXV1e0bdsWf//73+32ZuJ9vjc7duzA0KFDERgYCIVCgY0bN9qdr6v7evDgQQwYMAA6nQ5BQUGYP3/+/RcvqF6tW7dOaDQa8fnnn4vDhw+LSZMmCW9vb5GVlSV1aQ4jJiZGLFu2TGRkZIj9+/eLhx9+WLRs2VIUFBTY2jzzzDMiKChIJCYmitTUVNGnTx/Rt29f2/mKigrRtWtXER0dLdLT08XmzZuFr6+vmDNnjhSXJHt79+4VrVu3Ft27dxczZsywHed9vn+5ubmiVatWYvz48SI5OVmcOXNG/PTTT+LUqVO2Nm+99ZbQ6/Vi48aN4sCBA+KPf/yjCA4OFsXFxbY2Q4YMET169BB79uwR//3vf0W7du3EqFGjpLgkWXrzzTdF06ZNxaZNm8TZs2fFhg0bhIeHh3j//fdtbXif783mzZvFyy+/LL755hsBQHz77bd25+vivppMJuHv7y9Gjx4tMjIyxNq1a4Wrq6v45JNP7qt2hp56FhERIaZNm2b7vcViEYGBgSIhIUHCqhxbdna2ACB+/fVXIYQQeXl5wsXFRWzYsMHW5ujRowKASEpKEkJU/keqVCqF0Wi0tfn444+Fl5eXKC0tbdgLkLn8/HzRvn17sW3bNjFo0CBb6OF9rhsvvPCC6N+//23PW61WYTAYxIIFC2zH8vLyhFarFWvXrhVCCHHkyBEBQKSkpNjabNmyRSgUCpGZmVl/xTuQ2NhY8fTTT9sde/TRR8Xo0aOFELzPdeV/Q09d3dePPvpINGnSxO7vjRdeeEF07Njxvurl4616VFZWhrS0NERHR9uOKZVKREdHIykpScLKHJvJZAIA+Pj4AADS0tJQXl5ud59DQkLQsmVL231OSkpCt27d4O/vb2sTExMDs9mMw4cPN2D18jdt2jTExsba3U+A97mufP/99wgPD8fjjz8OPz8/9OzZE0uXLrWdP3v2LIxGo9191uv1iIyMtLvP3t7eCA8Pt7WJjo6GUqlEcnJyw12MjPXt2xeJiYk4ceIEAODAgQPYuXMnHnroIQC8z/Wlru5rUlISBg4cCI1GY2sTExOD48eP4/r16/dcHzccrUc5OTmwWCx2XwAA4O/vj2PHjklUlWOzWq2YOXMm+vXrh65duwIAjEYjNBoNvL297dr6+/vDaDTa2lT351B1jiqtW7cO+/btQ0pKyi3neJ/rxpkzZ/Dxxx9j1qxZeOmll5CSkoLnn38eGo0G48aNs92n6u7jzffZz8/P7rxarYaPjw/v8w0vvvgizGYzQkJCoFKpYLFY8Oabb2L06NEAwPtcT+rqvhqNRgQHB9/yM6rONWnS5J7qY+ghhzJt2jRkZGRg586dUpfS6Fy8eBEzZszAtm3boNPppC6n0bJarQgPD8c//vEPAEDPnj2RkZGBxYsXY9y4cRJX13h8+eWXWL16NdasWYMuXbpg//79mDlzJgIDA3mfnRgfb9UjX19fqFSqW2a3ZGVlwWAwSFSV45o+fTo2bdqEn3/+GS1atLAdNxgMKCsrQ15enl37m++zwWCo9s+h6hxVPr7Kzs5Gr169oFaroVar8euvv2LhwoVQq9Xw9/fnfa4DAQEB6Ny5s92xTp064cKFCwB+u093+nvDYDAgOzvb7nxFRQVyc3N5n2/461//ihdffBEjR45Et27dMGbMGPzpT39CQkICAN7n+lJX97W+/i5h6KlHGo0GYWFhSExMtB2zWq1ITExEVFSUhJU5FiEEpk+fjm+//Rbbt2+/pcszLCwMLi4udvf5+PHjuHDhgu0+R0VF4dChQ3b/oW3btg1eXl63fAE5q8GDB+PQoUPYv3+/7RUeHo7Ro0fb/jfv8/3r16/fLUsunDhxAq1atQIABAcHw2Aw2N1ns9mM5ORku/ucl5eHtLQ0W5vt27fDarUiMjKyAa5C/oqKiqBU2n/FqVQqWK1WALzP9aWu7mtUVBR27NiB8vJyW5tt27ahY8eO9/xoCwCnrNe3devWCa1WK5YvXy6OHDkiJk+eLLy9ve1mt9CdTZ06Vej1evHLL7+IK1eu2F5FRUW2Ns8884xo2bKl2L59u0hNTRVRUVEiKirKdr5qKvXvf/97sX//frF161bRrFkzTqW+i5tnbwnB+1wX9u7dK9RqtXjzzTfFyZMnxerVq4Wbm5tYtWqVrc1bb70lvL29xXfffScOHjwohg0bVu2U3549e4rk5GSxc+dO0b59e6efSn2zcePGiebNm9umrH/zzTfC19dXzJ4929aG9/ne5Ofni/T0dJGeni4AiH/+858iPT1dnD9/XghRN/c1Ly9P+Pv7izFjxoiMjAyxbt064ebmxinrjuCDDz4QLVu2FBqNRkRERIg9e/ZIXZJDAVDta9myZbY2xcXF4tlnnxVNmjQRbm5u4pFHHhFXrlyx+znnzp0TDz30kHB1dRW+vr7iz3/+sygvL2/gq3Es/xt6eJ/rxg8//CC6du0qtFqtCAkJEUuWLLE7b7Vaxauvvir8/f2FVqsVgwcPFsePH7drc+3aNTFq1Cjh4eEhvLy8RHx8vMjPz2/Iy5A1s9ksZsyYIVq2bCl0Op1o06aNePnll+2mQPM+35uff/652r+Tx40bJ4Sou/t64MAB0b9/f6HVakXz5s3FW2+9dd+1K4S4aXlKIiIiokaKY3qIiIjIKTD0EBERkVNg6CEiIiKnwNBDREREToGhh4iIiJwCQw8RERE5BYYeIiIicgoMPUREROQUGHqIiIjIKTD0EBERkVNg6CEiIiKnwNBDRERETuH/ASlS84I/VlYqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_model = torch.nn.Linear(2, 1)\n",
    "fake_optimizer = torch.optim.AdamW(fake_model.parameters(), lr=0.0001)\n",
    "fake_scheduler = torch.optim.lr_scheduler.OneCycleLR(fake_optimizer, max_lr=0.001, pct_start=0.05,\n",
    "                                                steps_per_epoch=50, epochs=20)\n",
    "lrs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    fake_optimizer.step()\n",
    "    lrs.append(fake_optimizer.param_groups[0][\"lr\"])\n",
    "    fake_scheduler.step()\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, pct_start=0.05,\n",
    "                                                steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6791b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4360236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.141746759414673;\n",
      "Loss: 8.615472731590271;\n",
      "Loss: 8.193301642735799;\n",
      "Loss: 7.868091729879379;\n",
      "Loss: 7.626520300865173;\n",
      "Loss: 7.44104191382726;\n",
      "Loss: 7.290940326281956;\n",
      "Loss: 7.171681789159774;\n",
      "Loss: 7.068861685329013;\n",
      "Loss: 6.979812559127808;\n",
      "Loss: 6.900501418980685;\n",
      "Loss: 6.827723447879156;\n",
      "Loss: 6.761177597779494;\n",
      "Loss: 6.701167850153786;\n",
      "Loss: 6.642160833358765;\n",
      "Loss: 6.590262079238892;\n",
      "Loss: 6.541541478213142;\n",
      "Loss: 6.493674022886488;\n",
      "Loss: 6.449504480361939;\n",
      "Loss: 6.406869073867798;\n",
      "Loss: 6.365596494901748;\n",
      "Loss: 6.326820824796504;\n",
      "Loss: 6.289682043946308;\n",
      "Loss: 6.253461649815241;\n",
      "Loss: 6.2178367319107055;\n",
      "Loss: 6.1845130916742175;\n",
      "Loss: 6.151026591548213;\n",
      "Loss: 6.119829986095429;\n",
      "Loss: 6.089503892701248;\n",
      "Loss: 6.060107823053996;\n",
      "Loss: 6.030702410667173;\n",
      "Loss: 6.003473145514727;\n",
      "Loss: 5.975772395856453;\n",
      "Loss: 5.948673771128935;\n",
      "Loss: 5.9233702989305765;\n",
      "Loss: 5.897626327408685;\n",
      "Loss: 5.872930125674686;\n",
      "Loss: 5.849010893671136;\n",
      "Loss: 5.8250460315362;\n",
      "Loss: 5.802195795416832;\n",
      "Loss: 5.779502040118706;\n",
      "Loss: 5.758050669488453;\n",
      "Loss: 5.735701867037041;\n",
      "Loss: 5.7145783943479715;\n",
      "Loss: 5.6943248054716324;\n",
      "Loss: 5.674654198107512;\n",
      "Loss: 5.65484307299269;\n",
      "Loss: 5.6350393857558565;\n",
      "Loss: 5.61641620811151;\n",
      "Loss: 5.597572368812561;\n",
      "Loss: 5.579767802275863;\n",
      "Loss: 5.561517491432337;\n",
      "Loss: 5.543740175715032;\n",
      "Loss: 5.526317595817424;\n",
      "Loss: 5.509278997507963;\n",
      "Loss: 5.492722331030028;\n",
      "Loss: 5.476083384229426;\n",
      "Loss: 5.459070507164659;\n",
      "Loss: 5.443333773774616;\n",
      "Loss: 5.4276641608874;\n",
      "Loss: 5.411824800303725;\n",
      "Loss: 5.397062848844836;\n",
      "Loss: 5.382181437242599;\n",
      "Loss: 5.367384492643177;\n",
      "Loss: 5.353551710935739;\n",
      "Loss: 5.3391602359396035;\n",
      "Loss: 5.3252406859397885;\n",
      "Loss: 5.311497021212297;\n",
      "Loss: 5.297878136773041;\n",
      "Loss: 5.284357776948384;\n",
      "Loss: 5.271639136361404;\n",
      "Loss: 5.259032690425713;\n",
      "Loss: 5.246589020376336;\n",
      "Loss: 5.23425736201776;\n",
      "Loss: 5.221964770730336;\n",
      "Loss: 5.209961047360771;\n",
      "Loss: 5.1982229829453805;\n",
      "Loss: 5.186505354856833;\n",
      "Loss: 5.174670895172071;\n",
      "Loss: 5.1630643015503885;\n",
      "Loss: 5.151056782169107;\n",
      "Loss: 5.140047034141494;\n",
      "Loss: 5.128610412471265;\n",
      "Loss: 5.1178498151188805;\n",
      "Loss: 5.1066185009900265;\n",
      "Loss: 5.095876668248065;\n",
      "Loss: 5.085401631525193;\n",
      "Loss: 5.0749621705575425;\n",
      "Loss: 5.064451308571891;\n",
      "Loss: 5.054303279108471;\n",
      "Loss: 5.04400324176956;\n",
      "Loss: 5.0338296502051145;\n",
      "Loss: 5.023868121895739;\n",
      "Loss: 5.013962204430966;\n",
      "Loss: 5.004325482569243;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First epoch - 3.9906687960624696, saving model..\n",
      "Epoch: 1, Train loss: 5.004, Val loss: 3.991,            Epoch time=1535.213s\n",
      "Hi !\n",
      "How are you ?\n",
      "How are you ?\n",
      "And then you ' re gonna get the hell out of these ,\n",
      "Loss: 4.0228636980056764;\n",
      "Loss: 4.009443733692169;\n",
      "Loss: 4.006328652699788;\n",
      "Loss: 4.00545658826828;\n",
      "Loss: 4.003859479427337;\n",
      "Loss: 4.001544729073842;\n",
      "Loss: 3.9987537830216544;\n",
      "Loss: 3.9961545234918594;\n",
      "Loss: 3.9939771585994297;\n",
      "Loss: 3.9899342179298403;\n",
      "Loss: 3.98518888798627;\n",
      "Loss: 3.980758506457011;\n",
      "Loss: 3.977944076061249;\n",
      "Loss: 3.9748573306628634;\n",
      "Loss: 3.9738698366483054;\n",
      "Loss: 3.9711970692873;\n",
      "Loss: 3.967009813364814;\n",
      "Loss: 3.9630364553133646;\n",
      "Loss: 3.959605048204723;\n",
      "Loss: 3.9561953246593475;\n",
      "Loss: 3.9524463454882306;\n",
      "Loss: 3.949324184764515;\n",
      "Loss: 3.945213615168696;\n",
      "Loss: 3.9421158228317896;\n",
      "Loss: 3.93793953666687;\n",
      "Loss: 3.934808613061905;\n",
      "Loss: 3.932689008977678;\n",
      "Loss: 3.928394382596016;\n",
      "Loss: 3.925597541085605;\n",
      "Loss: 3.9222169999281564;\n",
      "Loss: 3.919169738984877;\n",
      "Loss: 3.9169104511290787;\n",
      "Loss: 3.913351117408637;\n",
      "Loss: 3.9111156807226295;\n",
      "Loss: 3.9078206555502755;\n",
      "Loss: 3.905410893890593;\n",
      "Loss: 3.902830790957889;\n",
      "Loss: 3.90041522496625;\n",
      "Loss: 3.8973679662362124;\n",
      "Loss: 3.8943394152522086;\n",
      "Loss: 3.8909978509530787;\n",
      "Loss: 3.888351796127501;\n",
      "Loss: 3.885440221276394;\n",
      "Loss: 3.8823762074925683;\n",
      "Loss: 3.879783842722575;\n",
      "Loss: 3.8767000515046326;\n",
      "Loss: 3.8737852690067696;\n",
      "Loss: 3.871687628577153;\n",
      "Loss: 3.8686220094622397;\n",
      "Loss: 3.865904217195511;\n",
      "Loss: 3.8630283291199627;\n",
      "Loss: 3.8597872075667747;\n",
      "Loss: 3.856288311301537;\n",
      "Loss: 3.8540664841510632;\n",
      "Loss: 3.851059884938327;\n",
      "Loss: 3.8479544383713176;\n",
      "Loss: 3.8449695160096153;\n",
      "Loss: 3.842032467743446;\n",
      "Loss: 3.839392005467819;\n",
      "Loss: 3.8368230357567468;\n",
      "Loss: 3.834317620113248;\n",
      "Loss: 3.8315567143117226;\n",
      "Loss: 3.8290906243097216;\n",
      "Loss: 3.8262154721468686;\n",
      "Loss: 3.823835446907924;\n",
      "Loss: 3.8214165525725394;\n",
      "Loss: 3.818976909971949;\n",
      "Loss: 3.816533875816009;\n",
      "Loss: 3.814261740739795;\n",
      "Loss: 3.8115730117389135;\n",
      "Loss: 3.809432980712031;\n",
      "Loss: 3.8071669882204797;\n",
      "Loss: 3.8046330539820947;\n",
      "Loss: 3.8020964521330756;\n",
      "Loss: 3.799368042755127;\n",
      "Loss: 3.796502350443288;\n",
      "Loss: 3.794436371790898;\n",
      "Loss: 3.7918510174445617;\n",
      "Loss: 3.7891504725323446;\n",
      "Loss: 3.787110729187727;\n",
      "Loss: 3.7845020919964636;\n",
      "Loss: 3.782311207085121;\n",
      "Loss: 3.7798512962065547;\n",
      "Loss: 3.7776562175580435;\n",
      "Loss: 3.7752265784039216;\n",
      "Loss: 3.7725775849541954;\n",
      "Loss: 3.7702113194575255;\n",
      "Loss: 3.76773674688556;\n",
      "Loss: 3.7653356362996475;\n",
      "Loss: 3.7632822733190325;\n",
      "Loss: 3.7609031947628484;\n",
      "Loss: 3.758587916026945;\n",
      "Loss: 3.7560706937441264;\n",
      "Loss: 3.7539303558938046;\n",
      "Loss: 3.751544233372337;\n",
      "Improved from 3.9906687960624696 to 3.407601182460785, saving model..\n",
      "Epoch: 2, Train loss: 3.752, Val loss: 3.408,            Epoch time=1743.703s\n",
      "Hey !\n",
      "How ' s your point ?\n",
      "How ' s the wife ?\n",
      "You ' re still gonna get these French French French French , right ?\n",
      "Loss: 3.407296533584595;\n",
      "Loss: 3.422918517589569;\n",
      "Loss: 3.4215421946843465;\n",
      "Loss: 3.4159396052360536;\n",
      "Loss: 3.4169283151626586;\n",
      "Loss: 3.414939382870992;\n",
      "Loss: 3.4163484958239962;\n",
      "Loss: 3.4153176838159562;\n",
      "Loss: 3.4166637290848625;\n",
      "Loss: 3.4166196529865265;\n",
      "Loss: 3.415450099815022;\n",
      "Loss: 3.4149469033877056;\n",
      "Loss: 3.4128258417202875;\n",
      "Loss: 3.4140162570135932;\n",
      "Loss: 3.4142714414596558;\n",
      "Loss: 3.412486542314291;\n",
      "Loss: 3.4118417061076443;\n",
      "Loss: 3.4124070783456166;\n",
      "Loss: 3.411366949332388;\n",
      "Loss: 3.4113450585603715;\n",
      "Loss: 3.4096084494817824;\n",
      "Loss: 3.40846721443263;\n",
      "Loss: 3.4088250521991563;\n",
      "Loss: 3.40769117295742;\n",
      "Loss: 3.407409582424164;\n",
      "Loss: 3.4065160919152775;\n",
      "Loss: 3.406451397560261;\n",
      "Loss: 3.4045883338791985;\n",
      "Loss: 3.4025007744493156;\n",
      "Loss: 3.4016928470134733;\n",
      "Loss: 3.400484881093425;\n",
      "Loss: 3.4000371911376717;\n",
      "Loss: 3.3990262723691536;\n",
      "Loss: 3.3970312598172354;\n",
      "Loss: 3.3960633301734924;\n",
      "Loss: 3.3951912580596075;\n",
      "Loss: 3.3949418085330243;\n",
      "Loss: 3.3944758107160267;\n",
      "Loss: 3.3936798247924216;\n",
      "Loss: 3.3931041973233222;\n",
      "Loss: 3.3925270273627306;\n",
      "Loss: 3.3918893201010567;\n",
      "Loss: 3.3912755952324978;\n",
      "Loss: 3.390174272818999;\n",
      "Loss: 3.3891465858883327;\n",
      "Loss: 3.388393519287524;\n",
      "Loss: 3.3879188023729525;\n",
      "Loss: 3.3867464464406174;\n",
      "Loss: 3.385841792165017;\n",
      "Loss: 3.384770368337631;\n",
      "Loss: 3.383554452260335;\n",
      "Loss: 3.382069797194921;\n",
      "Loss: 3.381254393334659;\n",
      "Loss: 3.38021603429759;\n",
      "Loss: 3.378765039574016;\n",
      "Loss: 3.3777526163203375;\n",
      "Loss: 3.3762835293903684;\n",
      "Loss: 3.374800617448215;\n",
      "Loss: 3.373592573545747;\n",
      "Loss: 3.372677925268809;\n",
      "Loss: 3.3716484022922204;\n",
      "Loss: 3.3703878106224923;\n",
      "Loss: 3.3689737607562353;\n",
      "Loss: 3.3678946030139922;\n",
      "Loss: 3.36651529444181;\n",
      "Loss: 3.3651965339978536;\n",
      "Loss: 3.3638266374815755;\n",
      "Loss: 3.3623690510497375;\n",
      "Loss: 3.3610173094790916;\n",
      "Loss: 3.3598513508183614;\n",
      "Loss: 3.3588802951490377;\n",
      "Loss: 3.3579882848925062;\n",
      "Loss: 3.3568512288185017;\n",
      "Loss: 3.3558940586206076;\n",
      "Loss: 3.35463553498586;\n",
      "Loss: 3.353026319208898;\n",
      "Loss: 3.3523825357486676;\n",
      "Loss: 3.351635489280407;\n",
      "Loss: 3.3507043880450573;\n",
      "Loss: 3.3497399204075338;\n",
      "Loss: 3.3482960797239234;\n",
      "Loss: 3.3472772589253217;\n",
      "Loss: 3.3461175345512757;\n",
      "Loss: 3.3449084075859616;\n",
      "Loss: 3.3436918832274043;\n",
      "Loss: 3.3428314016586125;\n",
      "Loss: 3.3419165683614795;\n",
      "Loss: 3.340945250527425;\n",
      "Loss: 3.339905039433683;\n",
      "Loss: 3.3386028553644818;\n",
      "Loss: 3.337219183706975;\n",
      "Loss: 3.3359617358705274;\n",
      "Loss: 3.3348213791590866;\n",
      "Loss: 3.333392724255298;\n",
      "Loss: 3.331903861070934;\n",
      "Improved from 3.407601182460785 to 3.097726101398468, saving model..\n",
      "Epoch: 3, Train loss: 3.332, Val loss: 3.098,            Epoch time=1045.455s\n",
      "Hey !\n",
      "How ' s your doing ?\n",
      "How ' s the wife ?\n",
      "You ' re still gonna get these French fries , huh ?\n",
      "Loss: 3.0616030311584472;\n",
      "Loss: 3.0612175607681276;\n",
      "Loss: 3.074920494556427;\n",
      "Loss: 3.090023838877678;\n",
      "Loss: 3.0871779494285585;\n",
      "Loss: 3.0944527180989585;\n",
      "Loss: 3.0909730877195085;\n",
      "Loss: 3.0910813093185423;\n",
      "Loss: 3.0943890635172524;\n",
      "Loss: 3.096588472127914;\n",
      "Loss: 3.0967804685505955;\n",
      "Loss: 3.098459010322889;\n",
      "Loss: 3.0996320097263044;\n",
      "Loss: 3.0995007722718375;\n",
      "Loss: 3.0998186310132345;\n",
      "Loss: 3.100528836250305;\n",
      "Loss: 3.100866607357474;\n",
      "Loss: 3.1009369406435225;\n",
      "Loss: 3.100823798556077;\n",
      "Loss: 3.1008082830905916;\n",
      "Loss: 3.0995223011289323;\n",
      "Loss: 3.100391125137156;\n",
      "Loss: 3.1001877456126006;\n",
      "Loss: 3.099449102083842;\n",
      "Loss: 3.0998200699806215;\n",
      "Loss: 3.100294169004147;\n",
      "Loss: 3.1002002512084115;\n",
      "Loss: 3.100465443304607;\n",
      "Loss: 3.10096003491303;\n",
      "Loss: 3.100776960452398;\n",
      "Loss: 3.100630249361838;\n",
      "Loss: 3.0999983444064854;\n",
      "Loss: 3.099843130761927;\n",
      "Loss: 3.100002439653172;\n",
      "Loss: 3.098622492313385;\n",
      "Loss: 3.0980597050322425;\n",
      "Loss: 3.0973344143016917;\n",
      "Loss: 3.0963370791861884;\n",
      "Loss: 3.0954000263336376;\n",
      "Loss: 3.0954642415046694;\n",
      "Loss: 3.094708681629925;\n",
      "Loss: 3.093781477894102;\n",
      "Loss: 3.0937790916686834;\n",
      "Loss: 3.092958422790874;\n",
      "Loss: 3.0925025888548956;\n",
      "Loss: 3.092075923214788;\n",
      "Loss: 3.0919478339844564;\n",
      "Loss: 3.0912857862313587;\n",
      "Loss: 3.0910067527148186;\n",
      "Loss: 3.090991113948822;\n",
      "Loss: 3.0900393331752105;\n",
      "Loss: 3.089088688630324;\n",
      "Loss: 3.088293533640088;\n",
      "Loss: 3.0875865633840913;\n",
      "Loss: 3.0864768092849038;\n",
      "Loss: 3.0865677586197853;\n",
      "Loss: 3.08633657999206;\n",
      "Loss: 3.0862469942405304;\n",
      "Loss: 3.085639585074732;\n",
      "Loss: 3.084895199418068;\n",
      "Loss: 3.084519591487822;\n",
      "Loss: 3.0840003436611543;\n",
      "Loss: 3.083418703684731;\n",
      "Loss: 3.0822855231538413;\n",
      "Loss: 3.081589258634127;\n",
      "Loss: 3.0811798543641062;\n",
      "Loss: 3.0809270359864875;\n",
      "Loss: 3.080061464975862;\n",
      "Loss: 3.079634926284569;\n",
      "Loss: 3.0792219437190465;\n",
      "Loss: 3.078633399983527;\n",
      "Loss: 3.0780107870697977;\n",
      "Loss: 3.0770137334849736;\n",
      "Loss: 3.0764325718944137;\n",
      "Loss: 3.075475624593099;\n",
      "Loss: 3.0743456306269294;\n",
      "Loss: 3.0740631250901655;\n",
      "Loss: 3.073535086802947;\n",
      "Loss: 3.0728453862516187;\n",
      "Loss: 3.072176481872797;\n",
      "Loss: 3.0717174804357836;\n",
      "Loss: 3.0710717538798726;\n",
      "Loss: 3.0701833965979426;\n",
      "Loss: 3.069710049203464;\n",
      "Loss: 3.068619691989001;\n",
      "Loss: 3.067949360137762;\n",
      "Loss: 3.06726424951663;\n",
      "Loss: 3.066478944122791;\n",
      "Loss: 3.0656016244513267;\n",
      "Loss: 3.0647087581157684;\n",
      "Loss: 3.0642413375927853;\n",
      "Loss: 3.063664887936219;\n",
      "Loss: 3.063154595718589;\n",
      "Loss: 3.062433629264223;\n",
      "Loss: 3.0618675161662856;\n",
      "Improved from 3.097726101398468 to 2.886229190349579, saving model..\n",
      "Epoch: 4, Train loss: 3.062, Val loss: 2.886,            Epoch time=1043.494s\n",
      "Hi !\n",
      "How ' s your business ?\n",
      "How ' s the wife doing ?\n",
      "You ' re still gonna eat these French French fries , right ?\n",
      "Loss: 2.836821072101593;\n",
      "Loss: 2.84816260099411;\n",
      "Loss: 2.8594021264712017;\n",
      "Loss: 2.853507471084595;\n",
      "Loss: 2.8576363768577577;\n",
      "Loss: 2.862640696366628;\n",
      "Loss: 2.8678794847215925;\n",
      "Loss: 2.872319565117359;\n",
      "Loss: 2.8719228103425767;\n",
      "Loss: 2.873315382003784;\n",
      "Loss: 2.874746254790913;\n",
      "Loss: 2.8743223563830056;\n",
      "Loss: 2.873556497830611;\n",
      "Loss: 2.874130519458226;\n",
      "Loss: 2.874349828243256;\n",
      "Loss: 2.874563093185425;\n",
      "Loss: 2.8750492673761703;\n",
      "Loss: 2.874814184639189;\n",
      "Loss: 2.8742209873701396;\n",
      "Loss: 2.874447604775429;\n",
      "Loss: 2.8750889225233167;\n",
      "Loss: 2.8753401790965687;\n",
      "Loss: 2.875338912424834;\n",
      "Loss: 2.8751390594244004;\n",
      "Loss: 2.8752678791999817;\n",
      "Loss: 2.8752366091654853;\n",
      "Loss: 2.8758860754083706;\n",
      "Loss: 2.8764432050500597;\n",
      "Loss: 2.877000861003481;\n",
      "Loss: 2.8773237359523773;\n",
      "Loss: 2.87745402789885;\n",
      "Loss: 2.877178871780634;\n",
      "Loss: 2.877872656764406;\n",
      "Loss: 2.877107603339588;\n",
      "Loss: 2.876910164969308;\n",
      "Loss: 2.876746750473976;\n",
      "Loss: 2.8772145474923625;\n",
      "Loss: 2.8779139282201465;\n",
      "Loss: 2.8776821458034028;\n",
      "Loss: 2.877987366735935;\n",
      "Loss: 2.8780009031877287;\n",
      "Loss: 2.8783366523470195;\n",
      "Loss: 2.8775243866166402;\n",
      "Loss: 2.8770879089290444;\n",
      "Loss: 2.8769806882010567;\n",
      "Loss: 2.876918542229611;\n",
      "Loss: 2.8773073378015073;\n",
      "Loss: 2.8773828832308452;\n",
      "Loss: 2.8769585137951132;\n",
      "Loss: 2.8771231383800506;\n",
      "Loss: 2.8773574706152374;\n",
      "Loss: 2.8768299647477957;\n",
      "Loss: 2.8763938967686777;\n",
      "Loss: 2.876020819125352;\n",
      "Loss: 2.8759172274416143;\n",
      "Loss: 2.8760143954839026;\n",
      "Loss: 2.8751633125857303;\n",
      "Loss: 2.8749404040287283;\n",
      "Loss: 2.8744916755466137;\n",
      "Loss: 2.874124740521113;\n",
      "Loss: 2.874088023490593;\n",
      "Loss: 2.8739500993297944;\n",
      "Loss: 2.873685054287078;\n",
      "Loss: 2.873487411402166;\n",
      "Loss: 2.8731130648759695;\n",
      "Loss: 2.872599473360813;\n",
      "Loss: 2.8726823066597555;\n",
      "Loss: 2.8723696081427965;\n",
      "Loss: 2.8718631251653037;\n",
      "Loss: 2.871297360794885;\n",
      "Loss: 2.870826990805881;\n",
      "Loss: 2.8704049619701175;\n",
      "Loss: 2.8699092082781332;\n",
      "Loss: 2.8697938833687755;\n",
      "Loss: 2.869406510321299;\n",
      "Loss: 2.8693176006957106;\n",
      "Loss: 2.868778985351711;\n",
      "Loss: 2.8679477001153506;\n",
      "Loss: 2.8677323038366778;\n",
      "Loss: 2.867395169764757;\n",
      "Loss: 2.8671713975329456;\n",
      "Loss: 2.8670912909217;\n",
      "Loss: 2.8666825675677106;\n",
      "Loss: 2.866423438957759;\n",
      "Loss: 2.8657069749551662;\n",
      "Loss: 2.8652755026761874;\n",
      "Loss: 2.8645266294753413;\n",
      "Loss: 2.864365318173712;\n",
      "Loss: 2.8640038024709464;\n",
      "Loss: 2.8637151782777575;\n",
      "Loss: 2.86301780462265;\n",
      "Loss: 2.8627834368270375;\n",
      "Loss: 2.8625051611982366;\n",
      "Loss: 2.8620658338323555;\n",
      "Loss: 2.8619221273472433;\n",
      "Improved from 2.886229190349579 to 2.73464470911026, saving model..\n",
      "Epoch: 5, Train loss: 2.862, Val loss: 2.735,            Epoch time=1041.595s\n",
      "Hey !\n",
      "How ' s your business ?\n",
      "How ' s the wife like a kids ?\n",
      "You ' re gonna eat these French cakes , you ' re gonna eat the food , you know ?\n",
      "Loss: 2.658648784160614;\n",
      "Loss: 2.6649367201328276;\n",
      "Loss: 2.6719025659561155;\n",
      "Loss: 2.6810892283916474;\n",
      "Loss: 2.6801606578826904;\n",
      "Loss: 2.6871785310904186;\n",
      "Loss: 2.688478297846658;\n",
      "Loss: 2.6915042808651926;\n",
      "Loss: 2.6927667291959128;\n",
      "Loss: 2.6965932533741;\n",
      "Loss: 2.6985055886615408;\n",
      "Loss: 2.6993582826852798;\n",
      "Loss: 2.6996791238051197;\n",
      "Loss: 2.6994487476348876;\n",
      "Loss: 2.6997306900024416;\n",
      "Loss: 2.699844230413437;\n",
      "Loss: 2.700219225322499;\n",
      "Loss: 2.7004117935233647;\n",
      "Loss: 2.700766639583989;\n",
      "Loss: 2.7000496879816054;\n",
      "Loss: 2.7008485771360853;\n",
      "Loss: 2.7006398850137536;\n",
      "Loss: 2.700797318375629;\n",
      "Loss: 2.70117448925972;\n",
      "Loss: 2.7016831582069396;\n",
      "Loss: 2.702316470146179;\n",
      "Loss: 2.70274221084736;\n",
      "Loss: 2.7042802781718116;\n",
      "Loss: 2.704833392028151;\n",
      "Loss: 2.7053611343701682;\n",
      "Loss: 2.706293051473556;\n",
      "Loss: 2.7061092051118614;\n",
      "Loss: 2.706447446418531;\n",
      "Loss: 2.706869852963616;\n",
      "Loss: 2.7074394520350866;\n",
      "Loss: 2.7075736105442045;\n",
      "Loss: 2.70840477853208;\n",
      "Loss: 2.7089106627514488;\n",
      "Loss: 2.7098065562737292;\n",
      "Loss: 2.710159892320633;\n",
      "Loss: 2.710705054620417;\n",
      "Loss: 2.711317612216586;\n",
      "Loss: 2.7116213003979173;\n",
      "Loss: 2.7112587667595256;\n",
      "Loss: 2.711518718454573;\n",
      "Loss: 2.7115583589802617;\n",
      "Loss: 2.711916087891193;\n",
      "Loss: 2.7117742164433003;\n",
      "Loss: 2.711519085777049;\n",
      "Loss: 2.711138325691223;\n",
      "Loss: 2.710688143945208;\n",
      "Loss: 2.710891392643635;\n",
      "Loss: 2.7105316390181486;\n",
      "Loss: 2.7109840500354765;\n",
      "Loss: 2.7105454837625675;\n",
      "Loss: 2.7101209219012943;\n",
      "Loss: 2.7101265482316936;\n",
      "Loss: 2.7107473065524266;\n",
      "Loss: 2.7106782094502853;\n",
      "Loss: 2.7103427288134894;\n",
      "Loss: 2.7103870840541653;\n",
      "Loss: 2.7104240993915067;\n",
      "Loss: 2.709975415297917;\n",
      "Loss: 2.7099726827815176;\n",
      "Loss: 2.7101402847583476;\n",
      "Loss: 2.710222192966577;\n",
      "Loss: 2.70999462782447;\n",
      "Loss: 2.7100535633283505;\n",
      "Loss: 2.7102108024859772;\n",
      "Loss: 2.7096867631844113;\n",
      "Loss: 2.7097101407319726;\n",
      "Loss: 2.7100268881519636;\n",
      "Loss: 2.7098569916045827;\n",
      "Loss: 2.7094979103835852;\n",
      "Loss: 2.7096005766232807;\n",
      "Loss: 2.709603964655023;\n",
      "Loss: 2.70978481215316;\n",
      "Loss: 2.7097024713724087;\n",
      "Loss: 2.709554501213605;\n",
      "Loss: 2.709318378865719;\n",
      "Loss: 2.708956273691154;\n",
      "Loss: 2.7086447860264196;\n",
      "Loss: 2.7086515898589627;\n",
      "Loss: 2.708399504110927;\n",
      "Loss: 2.7081996210883648;\n",
      "Loss: 2.7085681364702623;\n",
      "Loss: 2.7083484062929264;\n",
      "Loss: 2.708086894127456;\n",
      "Loss: 2.708145749033167;\n",
      "Loss: 2.707717853969998;\n",
      "Loss: 2.7076777249902158;\n",
      "Loss: 2.7077401736249094;\n",
      "Loss: 2.7075494379125615;\n",
      "Loss: 2.7071083013047565;\n",
      "Loss: 2.707000042739667;\n",
      "Improved from 2.73464470911026 to 2.6202042355537416, saving model..\n",
      "Epoch: 6, Train loss: 2.707, Val loss: 2.620,            Epoch time=1042.065s\n",
      "Hey !\n",
      "How are you ?\n",
      "How ' s the wife ?\n",
      "You eat these little French sauce , eat the tea , drink .\n",
      "Loss: 2.5722657585144044;\n",
      "Loss: 2.5680552911758423;\n",
      "Loss: 2.562110268274943;\n",
      "Loss: 2.565564069747925;\n",
      "Loss: 2.5658046851158143;\n",
      "Loss: 2.5613501540819805;\n",
      "Loss: 2.5631912320000785;\n",
      "Loss: 2.565678233504295;\n",
      "Loss: 2.5671888342168594;\n",
      "Loss: 2.56737994325161;\n",
      "Loss: 2.569892307736657;\n",
      "Loss: 2.569146104156971;\n",
      "Loss: 2.568399122770016;\n",
      "Loss: 2.5684342393704824;\n",
      "Loss: 2.5689354112148286;\n",
      "Loss: 2.5700917785614728;\n",
      "Loss: 2.569732218419804;\n",
      "Loss: 2.570659003191524;\n",
      "Loss: 2.5718065543551196;\n",
      "Loss: 2.5713048382401467;\n",
      "Loss: 2.5723580603940146;\n",
      "Loss: 2.5734117443453184;\n",
      "Loss: 2.5733073739901835;\n",
      "Loss: 2.5739321865141394;\n",
      "Loss: 2.574817510843277;\n",
      "Loss: 2.576243753203979;\n",
      "Loss: 2.576222587382352;\n",
      "Loss: 2.5761017961587225;\n",
      "Loss: 2.576658971515195;\n",
      "Loss: 2.5780688488086065;\n",
      "Loss: 2.577066180898297;\n",
      "Loss: 2.578504913933575;\n",
      "Loss: 2.578067348906488;\n",
      "Loss: 2.578014654517174;\n",
      "Loss: 2.5785686170373645;\n",
      "Loss: 2.5791337520215247;\n",
      "Loss: 2.579633569620751;\n",
      "Loss: 2.5799594883228605;\n",
      "Loss: 2.580091824134191;\n",
      "Loss: 2.579870956629515;\n",
      "Loss: 2.580297979523496;\n",
      "Loss: 2.580585968238967;\n",
      "Loss: 2.580414533587389;\n",
      "Loss: 2.580712529827248;\n",
      "Loss: 2.5812431484063465;\n",
      "Loss: 2.580871993795685;\n",
      "Loss: 2.580946596099975;\n",
      "Loss: 2.5809228839725256;\n",
      "Loss: 2.580907288497808;\n",
      "Loss: 2.5813185033082964;\n",
      "Loss: 2.5814051999064054;\n",
      "Loss: 2.5814832533552097;\n",
      "Loss: 2.5815278981091843;\n",
      "Loss: 2.581434137887425;\n",
      "Loss: 2.5814592502550644;\n",
      "Loss: 2.5810979176631994;\n",
      "Loss: 2.581422890048278;\n",
      "Loss: 2.5815634146435507;\n",
      "Loss: 2.581832330691612;\n",
      "Loss: 2.582154140094916;\n",
      "Loss: 2.582172736593934;\n",
      "Loss: 2.5822394958042327;\n",
      "Loss: 2.582210781706704;\n",
      "Loss: 2.582431668471545;\n",
      "Loss: 2.582637768140206;\n",
      "Loss: 2.58230842841394;\n",
      "Loss: 2.5826019849883974;\n",
      "Loss: 2.5828394780264183;\n",
      "Loss: 2.582921068340108;\n",
      "Loss: 2.5829231942210877;\n",
      "Loss: 2.5833682878084585;\n",
      "Loss: 2.5829197041028076;\n",
      "Loss: 2.5828737581756016;\n",
      "Loss: 2.583145683826627;\n",
      "Loss: 2.5830956960837046;\n",
      "Loss: 2.583051563171964;\n",
      "Loss: 2.583058581120008;\n",
      "Loss: 2.583362077275912;\n",
      "Loss: 2.583479698926588;\n",
      "Loss: 2.5833973852545022;\n",
      "Loss: 2.5831714579352627;\n",
      "Loss: 2.5831971833327922;\n",
      "Loss: 2.583193761167756;\n",
      "Loss: 2.58310377096846;\n",
      "Loss: 2.58285074058701;\n",
      "Loss: 2.582822093173515;\n",
      "Loss: 2.582678314557021;\n",
      "Loss: 2.5827721773087977;\n",
      "Loss: 2.582836515595404;\n",
      "Loss: 2.582855471915669;\n",
      "Loss: 2.5828783034099327;\n",
      "Loss: 2.5827691004716833;\n",
      "Loss: 2.5825765482712817;\n",
      "Loss: 2.5821236297805257;\n",
      "Loss: 2.5821366567988147;\n",
      "Improved from 2.6202042355537416 to 2.5288938136100767, saving model..\n",
      "Epoch: 7, Train loss: 2.582, Val loss: 2.529,            Epoch time=1041.580s\n",
      "Hey !\n",
      "How are you ?\n",
      "How ' s the wife , how are you ?\n",
      "You ' re gonna eat these soft French French , eat some more tea .\n",
      "Loss: 2.436476068496704;\n",
      "Loss: 2.438237180709839;\n",
      "Loss: 2.437506172657013;\n",
      "Loss: 2.4356265687942504;\n",
      "Loss: 2.437560117721558;\n",
      "Loss: 2.437571989297867;\n",
      "Loss: 2.441468573978969;\n",
      "Loss: 2.441407589018345;\n",
      "Loss: 2.445848284562429;\n",
      "Loss: 2.445225159406662;\n",
      "Loss: 2.447540937120264;\n",
      "Loss: 2.4487915114561716;\n",
      "Loss: 2.450716550900386;\n",
      "Loss: 2.4527711996861865;\n",
      "Loss: 2.4529227923552197;\n",
      "Loss: 2.454782056435943;\n",
      "Loss: 2.4566419673667235;\n",
      "Loss: 2.4571495912472407;\n",
      "Loss: 2.4574883234500886;\n",
      "Loss: 2.458227644264698;\n",
      "Loss: 2.45893093137514;\n",
      "Loss: 2.4587433508851313;\n",
      "Loss: 2.458334544793419;\n",
      "Loss: 2.4584386475384234;\n",
      "Loss: 2.4594813480854034;\n",
      "Loss: 2.4595855420827863;\n",
      "Loss: 2.4602072215521775;\n",
      "Loss: 2.4610730729358536;\n",
      "Loss: 2.4623719395029133;\n",
      "Loss: 2.462541798869769;\n",
      "Loss: 2.4635039155329426;\n",
      "Loss: 2.4641340470686557;\n",
      "Loss: 2.464888869307258;\n",
      "Loss: 2.4646321555796793;\n",
      "Loss: 2.4646932592732567;\n",
      "Loss: 2.465053890844186;\n",
      "Loss: 2.4649729089801378;\n",
      "Loss: 2.465504234872366;\n",
      "Loss: 2.4658849606452846;\n",
      "Loss: 2.4668821274340154;\n",
      "Loss: 2.467493233477197;\n",
      "Loss: 2.467763223960286;\n",
      "Loss: 2.4682275535062304;\n",
      "Loss: 2.468508268621835;\n",
      "Loss: 2.4687344465520646;\n",
      "Loss: 2.4689932078641394;\n",
      "Loss: 2.4688440367262414;\n",
      "Loss: 2.4685654116918645;\n",
      "Loss: 2.468944113181562;\n",
      "Loss: 2.4691991113901137;\n",
      "Loss: 2.4695611712278103;\n",
      "Loss: 2.4701576527265403;\n",
      "Loss: 2.469866203497041;\n",
      "Loss: 2.4697939436524003;\n",
      "Loss: 2.4697205052809283;\n",
      "Loss: 2.4703382518036023;\n",
      "Loss: 2.470726724925794;\n",
      "Loss: 2.470799807556744;\n",
      "Loss: 2.4707568702051197;\n",
      "Loss: 2.471214783569177;\n",
      "Loss: 2.47182932902555;\n",
      "Loss: 2.472269631451176;\n",
      "Loss: 2.4725126694686828;\n",
      "Loss: 2.4727014064975084;\n",
      "Loss: 2.473145986978824;\n",
      "Loss: 2.473466748775858;\n",
      "Loss: 2.4734480050250665;\n",
      "Loss: 2.473196015866364;\n",
      "Loss: 2.473212672437447;\n",
      "Loss: 2.473294037103653;\n",
      "Loss: 2.4732460536083707;\n",
      "Loss: 2.473538148568736;\n",
      "Loss: 2.4733665803523914;\n",
      "Loss: 2.4731796484863438;\n",
      "Loss: 2.473245348405838;\n",
      "Loss: 2.473376450930771;\n",
      "Loss: 2.4737942349601103;\n",
      "Loss: 2.474302243529222;\n",
      "Loss: 2.4745642172384867;\n",
      "Loss: 2.4744280506819485;\n",
      "Loss: 2.4746848995744446;\n",
      "Loss: 2.4744497264594565;\n",
      "Loss: 2.4744162342778173;\n",
      "Loss: 2.474019606581756;\n",
      "Loss: 2.4742824122625238;\n",
      "Loss: 2.4742359356131662;\n",
      "Loss: 2.474142188025617;\n",
      "Loss: 2.4742230991748246;\n",
      "Loss: 2.4746630973360513;\n",
      "Loss: 2.4746797666417226;\n",
      "Loss: 2.474847274730494;\n",
      "Loss: 2.474957650653694;\n",
      "Loss: 2.475309938730732;\n",
      "Loss: 2.475366984770653;\n",
      "Loss: 2.475398773507068;\n",
      "Improved from 2.5288938136100767 to 2.461351625442505, saving model..\n",
      "Epoch: 8, Train loss: 2.475, Val loss: 2.461,            Epoch time=1041.491s\n",
      "Hey !\n",
      "How are you doing ?\n",
      "How are you , children ?\n",
      "You ' re gonna eat these soft French stick , eat the cup of tea , drink the tea .\n",
      "Loss: 2.3363753366470337;\n",
      "Loss: 2.3468039554357527;\n",
      "Loss: 2.3391212689876557;\n",
      "Loss: 2.3382519152760506;\n",
      "Loss: 2.339975601911545;\n",
      "Loss: 2.33892761250337;\n",
      "Loss: 2.3391106508459365;\n",
      "Loss: 2.3450196577608584;\n",
      "Loss: 2.346015915738212;\n",
      "Loss: 2.347650948166847;\n",
      "Loss: 2.3505669131062246;\n",
      "Loss: 2.3526797127723693;\n",
      "Loss: 2.3545217188505028;\n",
      "Loss: 2.355998873966081;\n",
      "Loss: 2.3566889384587606;\n",
      "Loss: 2.3571001087129115;\n",
      "Loss: 2.358726349367815;\n",
      "Loss: 2.3603751919666927;\n",
      "Loss: 2.359615971289183;\n",
      "Loss: 2.3598905823230742;\n",
      "Loss: 2.3602190097173055;\n",
      "Loss: 2.3602570219473407;\n",
      "Loss: 2.3604840679790664;\n",
      "Loss: 2.3606060113509497;\n",
      "Loss: 2.361820170879364;\n",
      "Loss: 2.3628001264425422;\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      7\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     10\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_generator, loss_fn, run)\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, scheduler, run, print_every)\u001b[0m\n\u001b[1;32m     20\u001b[0m B,S,C \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39mS, C), texts_ru_out\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39mS))\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler, run)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
    "\n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "\n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "\n",
    "    losses.append(val_loss)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
    "\n",
    "    print(translate(\"Привет!\"))\n",
    "    print(translate('Как твои дела?'))\n",
    "    print(translate('Как жена, как дети?'))\n",
    "    print(translate('съешь же ещё этих мягких французских булок, да выпей чаю'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df574987",
   "metadata": {},
   "source": [
    "### 8 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33addcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ' m gonna ask you all the time : do I know Tyler Drake ?\n"
     ]
    }
   ],
   "source": [
    "print(translate('Меня всё время спрашивают: знаю ли я Тайлера Дёрдена?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0c3847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('./data/en-ru-test.ru').read().replace('\\xa0', ' ')\n",
    "f = open('./data/en-ru-test.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b71254",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test_sents = open('./data/en-ru-test.en').read().splitlines()[666:777]  #  <-- Тут теперь настоящие английские предложения\n",
    "ru_test_sents = open('./data/en-ru-test.ru').read().splitlines()[666:777]  #  <-- А тут русские"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb82c021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 111)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_test_sents), len(ru_test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f850d553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"But we'll get through this, okay? I promise.\",\n",
       " 'Но мы как-нибудь выкрутимся.')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_test_sents[13], ru_test_sents[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2689e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "\n",
    "for ru_, en_ in zip(ru_test_sents, en_test_sents):\n",
    "    translations.append(\n",
    "        {\n",
    "            \"inpt\": ru_,\n",
    "            \"fact\": translate(ru_),\n",
    "            \"expt\": en_,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7ec47ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'Одним из элементов является программа интеграции дополнительного устройства для автоматической подачи листовой обложки T.I.P. E 1530.',\n",
       " 'fact': 'One element is the integration of additional device for automatic control of the T . I . P . 1530 .',\n",
       " 'expt': 'The T.I.P. E 1530 additional infeeder is one element for this purpose.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53687f7a",
   "metadata": {},
   "source": [
    "#### Нужно очистить всё от пунктуации и оставить только слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b83ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb2dab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(string):\n",
    "    translator = string.maketrans(\"\", \"\", punctuation)\n",
    "    return string.translate(translator).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eca5ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(translations)):\n",
    "    thriplet = translations[i]\n",
    "    thriplet[\"fact\"] = preproc(thriplet[\"fact\"])\n",
    "    thriplet[\"expt\"] = preproc(thriplet[\"expt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c646f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'Одним из элементов является программа интеграции дополнительного устройства для автоматической подачи листовой обложки T.I.P. E 1530.',\n",
       " 'fact': 'One element is the integration of additional device for automatic control of the T  I  P  1530',\n",
       " 'expt': 'The TIP E 1530 additional infeeder is one element for this purpose'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bf607ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'Но мы как-нибудь выкрутимся.',\n",
       " 'fact': 'But we  ll get some more',\n",
       " 'expt': 'But well get through this okay I promise'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfdf15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8dd79c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(translations)):\n",
    "    thriplet = translations[i]\n",
    "    fact = thriplet[\"fact\"].lower().split()\n",
    "    expt = thriplet[\"expt\"].lower().split()\n",
    "    thriplet[\"bleu\"] = nltk.translate.bleu_score.sentence_bleu([fact], expt, auto_reweigh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26cd1f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'Но мы как-нибудь выкрутимся.',\n",
       " 'fact': 'But we  ll get some more',\n",
       " 'expt': 'But well get through this okay I promise',\n",
       " 'bleu': 1.2882297539194154e-231}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddf3579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'inpt': 'Рабочая группа одобрила содержание проекта статьи 15 и передала его на рассмотрение редакционной группе.',\n",
       "  'fact': 'The Working Group approved the substance of draft article 15 and referred it to the drafting group',\n",
       "  'expt': 'The Working Group approved the substance of draft article 15 and referred it to the drafting group',\n",
       "  'bleu': 1.0},\n",
       " {'inpt': '98799', 'fact': '98799', 'expt': '98799', 'bleu': 1.0},\n",
       " {'inpt': 'Опубликовано: 24 Октябрь 2014',\n",
       "  'fact': 'Published  24 October 2014',\n",
       "  'expt': 'Published 24 October 2014',\n",
       "  'bleu': 1.0},\n",
       " {'inpt': 'Вы сказали - федеральные агенты?',\n",
       "  'fact': 'Did you say federal agents',\n",
       "  'expt': 'Did you say federal agents',\n",
       "  'bleu': 1.0},\n",
       " {'inpt': 'Чтобы заставить Нейтона ревновать.',\n",
       "  'fact': 'To make Nathan jealous',\n",
       "  'expt': 'Just to make Nathan jealous',\n",
       "  'bleu': 0.668740304976422},\n",
       " {'inpt': 'Click to see the amount of minerals in Fruits, vegetables, fats, cereals, spices and nuts rich in Phosphorus',\n",
       "  'fact': 'Click to see the amount of minerals in Fruits  vegetables  fats  cereals  legumes and spices rich in Phosphorus',\n",
       "  'expt': 'Click to see the amount of minerals in Fruits vegetables fats cereals spices and nuts rich in Vitamin B2',\n",
       "  'bleu': 0.6565037059458351},\n",
       " {'inpt': 'Я так счастлив быть сейчас с тобой',\n",
       "  'fact': 'I  m so happy to be with you now',\n",
       "  'expt': 'Im so happy to be with you right now',\n",
       "  'bleu': 0.6104735835807844},\n",
       " {'inpt': '58/99. Затрагивающие права человека действия Израиля в отношении палестинского народа на оккупированной палестинской территории, включая Восточный Иерусалим',\n",
       "  'fact': '58  99  The tragic human rights of Israel against the Palestinian people in the Occupied Palestinian Territory  including East Jerusalem',\n",
       "  'expt': '5899 Israeli practices affecting the human rights of the Palestinian people in the Occupied Palestinian Territory including East Jerusalem',\n",
       "  'bleu': 0.595092211343687},\n",
       " {'inpt': 'Из всех студентов в стране он выбрал нашу мисс Франклин.',\n",
       "  'fact': 'From all students in the country he chose our Ms  Franklin',\n",
       "  'expt': 'Of all the students in the country he chose our Miss Franklin',\n",
       "  'bleu': 0.5491004867761125},\n",
       " {'inpt': 'Я нашла её за зеркалом, в её квартире.',\n",
       "  'fact': 'I found her behind the mirror  in her apartment',\n",
       "  'expt': 'I found it behind the mirror in her flat',\n",
       "  'bleu': 0.5133450480401704}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(translations, key=lambda d: d[\"bleu\"], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f97dd",
   "metadata": {},
   "source": [
    "### Получилось более-менее. Можно попробовать попереводить что-то длинное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52207d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test_sents = open('./data/en-ru-test.en').read().splitlines()  #  <-- Тут теперь настоящие английские предложения\n",
    "ru_test_sents = open('./data/en-ru-test.ru').read().splitlines()  #  <-- А тут русские"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb206782",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_translate = []\n",
    "\n",
    "for i, sent in enumerate(ru_test_sents):\n",
    "    if len(sent) > 50:\n",
    "        to_translate.append(i)\n",
    "    if len(to_translate) == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc5e0f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48475fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "translations = []\n",
    "\n",
    "for i in to_translate:\n",
    "    ru_ = ru_test_sents[i]\n",
    "    en_ = en_test_sents[i]\n",
    "    translations.append(\n",
    "        {\n",
    "            \"inpt\": ru_,\n",
    "            \"fact\": preproc(translate(ru_)),\n",
    "            \"expt\": preproc(en_),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4007896b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'В ОАО «ТГК-1» подведены итоги производственной и финансово-хозяйственной деятельности за 9 месяцев 2009 года.',\n",
       " 'fact': 'The results of the production and financial sector in OJSC MMK amounted to the results of the year 2009',\n",
       " 'expt': 'JSC TGC1 releases its 3rdQuarter and 9 months 2010 financial results according to Russian accounting standards'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e92929e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(translations)):\n",
    "    thriplet = translations[i]\n",
    "    fact = thriplet[\"fact\"].lower().split()\n",
    "    expt = thriplet[\"expt\"].lower().split()\n",
    "    thriplet[\"bleu\"] = nltk.translate.bleu_score.sentence_bleu([fact], expt, auto_reweigh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bdbcb7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'В ОАО «ТГК-1» подведены итоги производственной и финансово-хозяйственной деятельности за 9 месяцев 2009 года.',\n",
       " 'fact': 'The results of the production and financial sector in OJSC MMK amounted to the results of the year 2009',\n",
       " 'expt': 'JSC TGC1 releases its 3rdQuarter and 9 months 2010 financial results according to Russian accounting standards',\n",
       " 'bleu': 1.0679799769055672e-231}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2af3abff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'inpt': 'Вьетнам озабочен фактом эскалации насилия в отношении невинных гражданских лиц.',\n",
       "  'fact': 'Viet Nam is concerned about the fact that violence against innocent civilians',\n",
       "  'expt': 'Viet Nam is concerned about the escalating violence against innocent civilians',\n",
       "  'bleu': 0.6407117598241614},\n",
       " {'inpt': 'Слушай, когда твой отец предложил мне место для того, чтобы остаться',\n",
       "  'fact': 'Listen  when your dad suggested me a place to stay',\n",
       "  'expt': 'Look when your dad offered me a place to stay',\n",
       "  'bleu': 0.5253819788848316},\n",
       " {'inpt': 'Мытищинский Район (Московская Область) 498684 **** Телефон',\n",
       "  'fact': 'Moscow Region  Moscow Region  498684  Phone',\n",
       "  'expt': 'Mytischinskiy District Moscow Region 498684  Phone',\n",
       "  'bleu': 0.5081327481546147},\n",
       " {'inpt': 'Совещание Сигнатариев Протокола по проблемам воды и здоровья к Конвенции',\n",
       "  'fact': 'Signatories to the Protocol on Water and Health Aspects of the Convention',\n",
       "  'expt': 'Meeting of the Signatories to the Protocol on Water',\n",
       "  'bleu': 0.473364131565402},\n",
       " {'inpt': 'невесты одеваются 5 out of 5 based on 3323 ratings.',\n",
       "  'fact': 'The bride  s blossoms 5 out of 5 based on 3323 ratings',\n",
       "  'expt': 'Bride Dressup 5 out of 5 based on 3321 ratings',\n",
       "  'bleu': 0.4548244246960421},\n",
       " {'inpt': 'Рассмотрение предложенных изменений и поправок к проекту текста Рабочей группы, содержащихся в докладах Специального комитета о работе его третьей сессии (A/AC.265/2004/5, приложение II) и четвертой сессии (A/59/360, приложение IV) и в предложениях, полученных Секретариатом от четвертой сессии.',\n",
       "  'fact': 'Consideration of proposed amendments and amendments to the draft text of the draft text of the Working Group contained in the reports of the Special Committee on its third session  A  AC  265  2004  5  annex II  and the fourth session',\n",
       "  'expt': 'Consideration of the proposed revisions and amendments to the draft text of the Working Group as contained in the reports of the Ad Hoc Committee on its third session AAC26520045 annex II and fourth session A59360 annex IV and in proposals received by the Secretariat from the fourth session',\n",
       "  'bleu': 0.4156497503481944},\n",
       " {'inpt': '← Результаты годового общего собрания акционеров «ЕВРАЗ Груп С.А.»',\n",
       "  'fact': 'The results of the annual general meeting of Shareholders of JSC  RSA',\n",
       "  'expt': '← Results of the Annual General Meeting',\n",
       "  'bleu': 0.3960914423407551},\n",
       " {'inpt': 'В первом полугодии их получили 117 425 беременных женщин.',\n",
       "  'fact': 'In the first half of them received 117 425 pregnant women',\n",
       "  'expt': 'In the first half of the year 117425 pregnant women received them',\n",
       "  'bleu': 0.38827267775222324},\n",
       " {'inpt': 'Делегация Соединенных Штатов заявила, что ее правительству потребуется больше информации о предлагаемой комиссии высокого уровня по расширению юридических прав и возможностей неимущих для того, чтобы она могла одобрить подобный шаг.',\n",
       "  'fact': 'The United States delegation stated that its Government would require more information on the proposed high  level Commission on the empowerment of law and the poor to enable it to adopt a decision',\n",
       "  'expt': 'OneThe delegation of the United States representative statedaid that theiher Government would require more information on the proposed High Level Commission on Legal Empowerment for the Poor before being in a position to endorse this activity',\n",
       "  'bleu': 0.3875091093026279},\n",
       " {'inpt': 'Выполнение этой задачи следует поручить созданной руководящей группе, состоящей из представителей пунктов связи.',\n",
       "  'fact': 'The tasks of this task should be set up to the steering group consisting of representatives from items',\n",
       "  'expt': 'This task should be given to the established steering group consisting of representatives of points of contact',\n",
       "  'bleu': 0.37570488258369017}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(translations, key=lambda d: d[\"bleu\"], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d409c",
   "metadata": {},
   "source": [
    "### Тут всё уже не так гладко."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa2e37b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'Для детей с дефектами умственного и физического развития, лишенных родительской опеки, сирот и брошенных детей, действует 56 интернатов, 6 детских домов, 2 дома ребенка.',\n",
       " 'fact': 'For children with defects and physical disabilities  deprived parental care  orphans and children  are employed by 56 schools  6 kindergartens  2 children',\n",
       " 'expt': 'There are 56 boarding schools six homes for overfives and two for underfives for children with physical or mental disabilities children lacking parental protection and orphaned and abandoned children',\n",
       " 'bleu': 2.266467503234495e-78}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727412b2",
   "metadata": {},
   "source": [
    "### Теперь можно доработать translate\n",
    "#### Для начала замерим старую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a48040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_translate = ru_test_sents[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa870ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!две раздельные комнаты и кухня, кондиционер, бойлер, Wi-Fi! подробнее...'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c42d7e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Несмотря на жесткую экономическую, торговую и финансовую блокаду, введенную в отношении Кубы правительством Соединенных Штатов более четырех десятилетий назад, Куба осуществляет и будет продолжать осуществлять программы сотрудничества со странами Африки, Карибского бассейна и другими братскими странами «третьего мира» в рамках совместных усилий, направленных на преодоление последствий работорговли и других печальных явлений колониализма и неоколониализма.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(to_translate, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75842951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 58s, sys: 1.33 s, total: 26min 59s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(10):\n",
    "    for sent in to_translate:\n",
    "        translate(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de917a",
   "metadata": {},
   "source": [
    "#### Теперь можно попробовать пооптимизировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ecb14b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def batch_translate(texts):\n",
    "\n",
    "\n",
    "    input_ids = [\n",
    "        tokenizer_en.encode(text).ids[:max_len_en]\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "    output_ids = [\n",
    "        [tokenizer_ru.token_to_id('[BOS]')]\n",
    "        for text\n",
    "        in texts\n",
    "    ]\n",
    "\n",
    "    input_ids_pads = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.LongTensor(ids) for ids in input_ids],\n",
    "        batch_first=True\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    output_ids_pads = torch.nn.utils.rnn.pad_sequence(\n",
    "        [torch.LongTensor(ids) for ids in output_ids],\n",
    "        batch_first=True\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    src_padding_mask = (input_ids_pads == PAD_IDX).to(DEVICE)\n",
    "    tgt_padding_mask = (output_ids_pads == PAD_IDX).to(DEVICE)\n",
    "\n",
    "    # Идея в том, чтобы на каждом витке вопхать в модель сразу весь многомерный тензор и потом посмотреть, какие предложения закончились\n",
    "    finished = [False for _ in range(len(texts))]\n",
    "    logits = model(input_ids_pads, output_ids_pads, src_padding_mask, tgt_padding_mask)\n",
    "    preds = logits.argmax(2)\n",
    "\n",
    "    while len(max(output_ids)) < 100 and not all(finished):\n",
    "        for i in range(len(finished)):\n",
    "            if not finished[i]:\n",
    "                item = preds[i].item()\n",
    "                if item in {tokenizer_ru.token_to_id('[EOS]'), tokenizer_ru.token_to_id('[PAD]')}:\n",
    "                    finished[i] = True\n",
    "                else:\n",
    "                    output_ids[i].append(item)\n",
    "\n",
    "        output_ids_pads = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.LongTensor(ids) for ids in output_ids],\n",
    "            batch_first=True\n",
    "        ).to(DEVICE)\n",
    "        tgt_padding_mask = (output_ids_pads == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids_pads, output_ids_pads, src_padding_mask, tgt_padding_mask)\n",
    "        preds = logits[:, -1, :].argmax(1)  # <-- я не понимаю почему это работает\n",
    "\n",
    "    return [\n",
    "        tokenizer_ru.decoder.decode(\n",
    "            [\n",
    "                tokenizer_ru.id_to_token(i)\n",
    "                for i\n",
    "                in ids[1:]\n",
    "            ]\n",
    "        )\n",
    "        for ids\n",
    "        in output_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b944f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Закон об объединениях, глава 66, (1972 год) требует, чтобы любой клуб, компания, товарищество или ассоциация, насчитывающие 10 или более человек, за исключением тех случаев, которые предусмотрены в разделе 2 этого закона, были внесены в регистр объединений.\tThe Associations Act , chapter 66 ( 1972 ) requires that any club , comrades or Associations , association or association , 10 or more than those in section 2 of the law , which are provided for in section 2 of the law , which are not\n",
      "По мнению половины опрошенных казахстанских руководителей высшего звена, состояние мировой экономики в течение ближайших 12 месяцев не изменится.\tAccording to the last half of the past year , the highest ranking of the world economy in the next 12 months will not change .\n",
      "Коллекция администрации Адамса.\tThe Paris administration of the Adam administration .\n",
      "Экономичное средство измерения расхода\tCapacity - building\n",
      "Tuyo Movil 571 *****\tTuo Mol 571 *****\n",
      "Если вы точно уверены, что не продвинетесь выше в этом сезоне, то может оказаться очень полезно ближе к концу сезона нанять себе очень хорошего пилота, так как можно не обращать внимание на затраты за предложения контрактов и премии за их подписание – деньги всё равно вернутся в исходное состояние и вы останетесь с этим же пилотом в следующем сезоне. Подробнее об этом в следующем разделе.\tIf you are sure that you will not be able to reach above the season , it may be very useful to the end of the season to hire a very good beer , as you can not pay attention to the costs of the contract and for\n",
      "Есть охота.\tThere ' s a hunter .\n",
      "Ты в больнице носа не показываешь?\tYou ' re not in the hospital ?\n",
      "Этот тот, кто сделал это с тобой.\tThat one who did it to you .\n",
      "http://www.culturalnet.ru/f/viewtopic.php?id=1611\thttp :// www . culturalnet . ru / f . php ? t = 1611\n"
     ]
    }
   ],
   "source": [
    "for ru_, en_ in zip(to_translate[10:20], batch_translate(to_translate[10:20])):\n",
    "    print(ru_, en_, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e3a5fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 223 ms, total: 1min 34s\n",
      "Wall time: 9.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(10):\n",
    "    batch_translate(to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a80c6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test_sents = open('./data/en-ru-test.en').read().splitlines()\n",
    "ru_test_sents = open('./data/en-ru-test.ru').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "faf1bdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ru_test_sents), len(en_test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b16a61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 54s, sys: 2.56 s, total: 3min 56s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "translations = []\n",
    "batchsize = 100\n",
    "\n",
    "for i in range(0, len(ru_test_sents), batchsize):\n",
    "    j = i + batchsize\n",
    "\n",
    "    translations.extend(\n",
    "        batch_translate(ru_test_sents[i:j])\n",
    "    )\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8f4493cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ru_test_sents), len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "209a80e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'И как ты только справляешься, папа, таская эти коробки взад-вперед целый день.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_test_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c2aacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for i in range(len(to_translate)):\n",
    "    ru_ = ru_test_sents[i]\n",
    "    en_ = en_test_sents[i]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"inpt\": ru_,\n",
    "            \"fact\": preproc(translations[i]),\n",
    "            \"expt\": preproc(en_),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60076db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'И как ты только справляешься, папа, таская эти коробки взад-вперед целый день.',\n",
       " 'fact': 'And as soon as you  re doing  Dad  these boxes are gonna go ahead',\n",
       " 'expt': 'I dont know how you do it Pop carrying these boxes around every day'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8c7fafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/smertlove/sandbox/hse/nlp_hw/compling_nlp_hw/.venv/lib/python3.10/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(metrics)):\n",
    "    thriplet = metrics[i]\n",
    "    fact = thriplet[\"fact\"].lower().split()\n",
    "    expt = thriplet[\"expt\"].lower().split()\n",
    "    thriplet[\"bleu\"] = nltk.translate.bleu_score.sentence_bleu([fact], expt, auto_reweigh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "96e9eb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inpt': 'И как ты только справляешься, папа, таская эти коробки взад-вперед целый день.',\n",
       " 'fact': 'And as soon as you  re doing  Dad  these boxes are gonna go ahead',\n",
       " 'expt': 'I dont know how you do it Pop carrying these boxes around every day',\n",
       " 'bleu': 5.3448396730677054e-155}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34f36a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'inpt': '- Боже, только послушайте себя.',\n",
       "  'fact': 'Oh  God  listen to yourself',\n",
       "  'expt': 'Oh God listen to yourself',\n",
       "  'bleu': 1.0},\n",
       " {'inpt': '12844', 'fact': '12844', 'expt': '12844', 'bleu': 1.0},\n",
       " {'inpt': 'Дракон 5 out of 5 based on 1479 ratings.',\n",
       "  'fact': 'Dragons 5 out of 5 based on 1479 ratings',\n",
       "  'expt': 'Dragon 5 out of 5 based on 1464 ratings',\n",
       "  'bleu': 0.6104735835807844},\n",
       " {'inpt': 'Я тебя люблю, папа.',\n",
       "  'fact': 'I love you  Dad',\n",
       "  'expt': 'I love you Dad How great',\n",
       "  'bleu': 0.5081327481546147},\n",
       " {'inpt': 'Now, if you have kids yourself, you know what--',\n",
       "  'fact': 'Now  if you have a decent  you know what',\n",
       "  'expt': 'Now if you have kids yourself you know what',\n",
       "  'bleu': 0.43167001068522526},\n",
       " {'inpt': 'Рассмотрение предложенных изменений и поправок к проекту текста Рабочей группы, содержащихся в докладах Специального комитета о работе его третьей сессии (A/AC.265/2004/5, приложение II) и четвертой сессии (A/59/360, приложение IV) и в предложениях, полученных Секретариатом от четвертой сессии.',\n",
       "  'fact': 'Consideration of proposed amendments and amendments to the draft text of the draft text of the Working Group contained in the reports of the Special Committee on its third session  A  AC  265  2004  5  annex II  and the fourth session',\n",
       "  'expt': 'Consideration of the proposed revisions and amendments to the draft text of the Working Group as contained in the reports of the Ad Hoc Committee on its third session AAC26520045 annex II and fourth session A59360 annex IV and in proposals received by the Secretariat from the fourth session',\n",
       "  'bleu': 0.4156497503481944},\n",
       " {'inpt': 'В первом полугодии их получили 117 425 беременных женщин.',\n",
       "  'fact': 'In the first half of them received 117 425 pregnant women',\n",
       "  'expt': 'In the first half of the year 117425 pregnant women received them',\n",
       "  'bleu': 0.38827267775222324},\n",
       " {'inpt': 'Выполнение этой задачи следует поручить созданной руководящей группе, состоящей из представителей пунктов связи.',\n",
       "  'fact': 'The tasks of this task should be set up to the steering group consisting of representatives from items',\n",
       "  'expt': 'This task should be given to the established steering group consisting of representatives of points of contact',\n",
       "  'bleu': 0.37570488258369017},\n",
       " {'inpt': '!две раздельные комнаты и кухня, кондиционер, бойлер, Wi-Fi! подробнее...',\n",
       "  'fact': 'Two bathrooms and kitchen  air conditioner  boiler  Wi  Fi  Wi  Fi',\n",
       "  'expt': 'two separate rooms and kitchen air conditioner boiler WiFi read more',\n",
       "  'bleu': 0.3672056269893592},\n",
       " {'inpt': 'Почему женщины всегда думают, что настоящая любовь - это должно быть напряженное путешествие, как у Ромео и Джульетты?',\n",
       "  'fact': 'Why would women always think that true love is supposed to be a tough trip like Romeo and Juliet',\n",
       "  'expt': 'Why do women always think that true love must be heavy drama in the style of Romeo and Juliet',\n",
       "  'bleu': 0.33380800216772966}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(metrics, key=lambda d: d[\"bleu\"], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa93d6",
   "metadata": {},
   "source": [
    "\n",
    "## Задание 2 (2 балла).\n",
    "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/13.pdf\n",
    "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как ее применить к паре en->ru на данных из семинара. Сколько моделей понадобится? Сколько запусков обучения нужно будет сделать?\n",
    "\n",
    "Ответ должен содержать как минимум 10 предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bf2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82543fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
